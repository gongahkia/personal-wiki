<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="Wiki Note: Large Language Models - Gabriel Ong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="../style.css">
  <link rel="preload" href="https://res.hajimehoshi.com/fonts/SuisseIntl-Regular-WebXL.woff2" as="font" crossorigin="anonymous">
  <link rel="preload" href="https://res.hajimehoshi.com/fonts/SuisseIntlMono-Regular-WebXL.woff2" as="font" crossorigin="anonymous">
  <style>
    .thin-space:after{content:"\2006"}
    pre {
      overflow-x: auto;
      max-width: 100%;
    }
  </style>
  <script src="../script.js" defer></script>
  <title>GABRIEL ONG</title>
  <link rel="shortcut icon" href="../asset/blob.ico" type="image/x-icon">
</head>
<body>
  <div id="click-container"></div>
  <input type="button" id="dark-mode">
  <label for="dark-mode">
    <img id="infinityButton" src="../asset/roller.png" height="24" width="24"/>
  </label>

  <main>
    <article class="overallArticleTags">

      <section class="note-header">
        <h2>Large Language Models</h2>

        <dl>
          <dt>File size</dt>
          <dd>9.4KB</dd>

          <dt>Lines of code</dt>
          <dd>223</dd>
        </dl>
      </section>

      <section class="note-content">
        <h1><code>Large Language Models</code></h1>
<p>Also known as LLMs.</p>
<h2>Theory</h2>
<ul>
<li>Tokenizer: splits strings into tokens</li>
<li>Lemmatization: reverts tokens to their root forms</li>
<li>Encoder: makes each token a vector via embedding, shifts weights to capture meaning/context of tokens via attention</li>
<li>Decoder: generates new text using encoded representations to predict next token in a sequence, can access current and previous encoder outputs</li>
<li>Transformers: Encoder + Decoder</li>
</ul>
<h2>Quickstart</h2>
<p>LLMs are typically built upon the following infrastructure.</p>
<ol>
<li><a href="https://huggingface.co/docs/text-generation-inference/en/index">Text Generation Interface (TGI)</a>: framework that features optimized transformer code, quantization, accelerated weight loading and logits warping</li>
<li><a href="https://github.com/huggingface/transformers">Hugging Face Transformers (HF)</a>: open-source library that provides many pre-trained models for NLP and other custom tasks</li>
<li><a href="https://github.com/vllm-project/vllm">Versatile Large Language Model (vLLM)</a>: framework that features efficient memory management with paged attention, optimized CUDA kernels, decoding algorithms and high-performance serving throughput <em>(significantly outperforming TGI and HF)</em></li>
</ol>
<p>You can <a href="#train-your-own-model">train your own model</a> or <a href="#use-a-prebuilt-model">use existing ones</a>.</p>
<h2>Train your own model</h2>
<p>First, learn <a href="https://learnxinyminutes.com/docs/python/">Python</a>.</p>
<p>Optionally, learn <a href="https://learnxinyminutes.com/docs/r/">R</a>, <a href="https://learnxinyminutes.com/docs/julia/">Julia</a>, <a href="https://learnxinyminutes.com/docs/c++/">C++</a>, <a href="https://learnxinyminutes.com/docs/scala/">Scala</a> and <a href="https://learnxinyminutes.com/docs/go/">Go</a>.</p>
<p>Then choose a library from below.</p>
<h3>Libraries</h3>
<h4>Python</h4>
<ul>
<li><a href="https://pytorch.org/">PyTorch</a></li>
<li><a href="https://www.tensorflow.org/">TensorFlow</a></li>
<li><a href="https://ollama.com/">Ollama</a></li>
<li><a href="https://huggingface.co/docs/transformers/en/index">Hugging Face Transformers</a></li>
<li><a href="https://www.nltk.org/">Natural Language Toolkit</a></li>
<li><a href="https://spacy.io/">spaCy</a></li>
<li><a href="https://github.com/PygmalionAI/aphrodite-engine">Aphrodite Engine</a></li>
</ul>
<h4>R</h4>
<ul>
<li><a href="https://www.tidyverse.org/packages/">Tidyverse</a></li>
<li><a href="https://topepo.github.io/caret/">caret</a></li>
<li><a href="https://cran.r-project.org/web/packages/tm/index.html">tm</a> </li>
</ul>
<h4>Julia</h4>
<ul>
<li><a href="https://fluxml.ai/Flux.jl/stable/">Flux</a></li>
<li><a href="https://juliaai.github.io/MLJ.jl/dev/">MLJ</a></li>
</ul>
<h4>C++</h4>
<ul>
<li><a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a> </li>
<li><a href="http://dlib.net/">dlib</a></li>
<li><a href="https://www.tensorflow.org/api_docs/cc">TensorFlow core</a></li>
</ul>
<h4>Scala</h4>
<ul>
<li><a href="https://spark.apache.org/mllib/">MLlib</a> </li>
<li><a href="https://github.com/scalanlp/breeze">Breeze</a> </li>
<li><a href="https://deeplearning4j.konduit.ai/">DeepLearning4j</a> </li>
<li><a href="https://www.weka.io/">Weka</a></li>
</ul>
<h4>Go</h4>
<ul>
<li><a href="https://github.com/knights-analytics/hugot">Hugot</a></li>
<li><a href="https://gorgonia.org/">Gorgonia</a></li>
</ul>
<h4>Language-agnostic</h4>
<ul>
<li><a href="https://ollama.com/">Ollama</a></li>
<li><a href="https://github.com/theroyallab/tabbyAPI">TabbyAPI</a></li>
</ul>
<h2>Use a prebuilt model</h2>
<p>In 2024, there are many existing LLM implementations to choose from. The more prominent ones have been listed below.</p>
<ul>
<li><a href="https://openai.com/index/gpt-4/">GPT-4</a><ul>
<li>OpenAI-developed</li>
<li>excellent at generating human-like text</li>
</ul>
</li>
<li><a href="https://research.google/pubs/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/">Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li>Google-developed</li>
<li>transformer-based model for a variety of NLP tasks</li>
</ul>
</li>
<li><a href="https://huggingface.co/docs/accelerate/en/usage_guides/megatron_lm">Megatron-LM</a><ul>
<li>NVIDIA-developed</li>
<li>scalable LLM framework for training and deploying models</li>
</ul>
</li>
<li><a href="https://llama.meta.com/">Llama</a><ul>
<li>Meta-developed</li>
<li>family of large language models for NLP and text generation </li>
</ul>
</li>
<li><a href="https://ai.meta.com/tools/fairseq/">Fairseq</a><ul>
<li>Meta-developed <em>(Facebook AI Research (FAIR))</em></li>
<li>sequence-to-sequence learning toolkit for training and deploying models </li>
<li>used for translation, summarization and language modeling</li>
</ul>
</li>
<li><a href="https://allenai.org/allennlp">AllenNLP</a><ul>
<li>Allen Institute for AI-developed</li>
<li>open-source library built on PyTorch</li>
<li>used for NLP research and deploying NLP-focused models</li>
</ul>
</li>
<li><a href="https://huggingface.co/docs/transformers/en/model_doc/t5">Text-To-Text Transfer Transformer (T5)</a><ul>
<li>Google-developed </li>
<li>highly versatile model that frames all NLP tasks as converting text to text</li>
</ul>
</li>
<li><a href="http://research.baidu.com/Blog/index-view?id=183">Enhanced Representation through Knowledge Integration (ERNIE)</a> <ul>
<li>Baidu-developed</li>
<li>incorporates knowledge graphs into the pre-training process for enhanced language comprehension</li>
</ul>
</li>
<li><a href="https://paperswithcode.com/method/ulmfit">Universal Language Model Fine-tuning (ULMFiT)</a><ul>
<li>fast.ai-developed</li>
<li>technique for fine-tuning pre-trained LLMs on downstream tasks</li>
</ul>
</li>
<li><a href="https://huggingface.co/bigcode/starcoder">StarCoder</a><ul>
<li>BigCode-developed</li>
<li>optimized for code generation and completion</li>
</ul>
</li>
<li><a href="https://huggingface.co/bigscience/bloom">BLOOM</a><ul>
<li>BigScience-developed</li>
<li>multilingual language model trained on many languages and tasks</li>
</ul>
</li>
<li><a href="https://huggingface.co/docs/transformers/en/model_doc/gpt_neox">GPT-NeoX</a><ul>
<li>EleutherAI-developed</li>
<li>designed to replicate GPT-3 architecture for various NLP tasks</li>
</ul>
</li>
<li><a href="https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling">Pythia</a><ul>
<li>EleutherAI-developed</li>
<li>family of models for large-scale NLP tasks</li>
</ul>
</li>
<li><a href="https://huggingface.co/OpenAssistant">OpenAssistant</a><ul>
<li>LAION-developed</li>
<li>conversational assistant for interactive AI dialogue capabilities</li>
</ul>
</li>
<li><a href="https://huggingface.co/databricks/dolly-v2-12b">Dolly V2</a><ul>
<li>Databricks-developed</li>
<li>high-performance commercial model for instruction-following tasks</li>
</ul>
</li>
<li><a href="https://github.com/Stability-AI/StableLM">StableLM</a><ul>
<li>Stability AI-developed</li>
<li>robust model for NLP tasks</li>
</ul>
</li>
<li><a href="https://localai.io/">LocalAI</a><ul>
<li>LocalAI-developed</li>
<li>facilitates local deployment of existing LLMs without relying on cloud-based services</li>
</ul>
</li>
</ul>
<h2>More on</h2>
<ul>
<li><a href="https://ollama.com/">Ollama</a></li>
<li><a href="https://huggingface.co/">huggingface.co</a></li>
<li><a href="./MachineLearning.md">MachineLearning.md</a></li>
<li><a href="https://github.com/eugeneyan/open-llms">Open LLMs</a> Github repository</li>
<li><a href="https://github.com/Hannibal046/Awesome-LLM">Awesome-LLM</a> Github repository</li>
<li><a href="https://medium.com/@rohit.k/tgi-vs-vllm-making-informed-choices-for-llm-deployment-37c56d7ff705">TGI vs vLLM</a> by Rohit Kewalramani</li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1cb8i7f/which_is_faster_vllm_tgi_or_tensorrt/">Which is faster, vLLM, TGI or TensorRT?</a> Reddit post</li>
</ul>
      </section>

    </article>

    <footer>
      <p>Â© 2023-<span id="current-year"></span> Gabriel Ong. All rights reserved.</p>
    </footer>
  </main>

  <div class="wrapper"></div>
</body>
</html>
