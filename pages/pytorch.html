<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="Wiki Note: PyTorch - Gabriel Ong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="../style.css">
  <link rel="preload" href="https://res.hajimehoshi.com/fonts/SuisseIntl-Regular-WebXL.woff2" as="font" crossorigin="anonymous">
  <link rel="preload" href="https://res.hajimehoshi.com/fonts/SuisseIntlMono-Regular-WebXL.woff2" as="font" crossorigin="anonymous">
  <style>
    .thin-space:after{content:"\2006"}
    pre {
      overflow-x: auto;
      max-width: 100%;
    }
  </style>
  <script src="../script.js" defer></script>
  <title>GABRIEL ONG</title>
  <link rel="shortcut icon" href="../asset/blob.ico" type="image/x-icon">
</head>
<body>
  <div id="click-container"></div>
  <input type="button" id="dark-mode">
  <label for="dark-mode">
    <img id="infinityButton" src="../asset/roller.png" height="24" width="24"/>
  </label>

  <main>
    <article class="overallArticleTags">

      <section class="note-header">
        <h2>PyTorch</h2>

        <dl>
          <dt>File size</dt>
          <dd>54.5KB</dd>

          <dt>Lines of code</dt>
          <dd>491</dd>
        </dl>
      </section>

      <section class="note-content">
        <h1><code>PyTorch</code></h1>
<p>Deep learning in Python.</p>
<h2>Introduction</h2>
<h3>Definitions</h3>
<ol>
<li>Deep learning: program infers relationships between user-defined <em>inputs</em> and <em>outputs</em> (supervised learning)</li>
<li>Deep learning is good for <ul>
<li>problems with long lists of rules that would be difficult to hardcode</li>
<li>continually changing environments </li>
<li>large unstructured datasets with unclear patterns</li>
</ul>
</li>
<li>Deep learning is mostly achieved through a multilayered neural network <em>(hence deep)</em></li>
<li>Neural networks work by<ol>
<li><strong>Input layer</strong>: <em>inputs</em> are numerically encoded into multi-dimensional vectors</li>
<li><strong>Hidden layer</strong>: vectors are processed by the many hidden layers, program finds patterns in the vectors</li>
<li><strong>Output layer</strong>: <em>output</em> is returned in the form of multi-dimensional vectors</li>
</ol>
</li>
<li>There are multiple types of neural networks<ul>
<li>convolutional neural network (images)</li>
<li>transformer (NLP)</li>
</ul>
</li>
<li>There are a few kinds of learning<ul>
<li>supervised learning: BOTH <em>inputs</em> and <em>outputs</em> are specified for the program</li>
<li>unsupervised / self-supervised learning: ONLY <em>input</em> is specified for the program</li>
<li>transfer learning: one program's <em>output</em> is FED to another program as <em>input</em></li>
<li>reinforcement learning: program is REWARDED for <em>ideal</em> behaviour and discouraged from <em>unideal</em> behaviour</li>
</ul>
</li>
<li>General workflow of building a deep learning program is<ol>
<li>convert raw data to <em>input tensors</em></li>
<li>build a model<ul>
<li>pick a loss function and optimizer</li>
<li>create the training loop</li>
</ul>
</li>
<li>tweak the model to fit the <em>data</em> and make a prediction with <em>output tensors</em></li>
<li>evaluate the model</li>
<li>improve the model through iterative experimentation</li>
<li>save and reload</li>
</ol>
</li>
<li>Tensor: any numerical representation of data <em>(most commonly multi-dimensional vectors)</em></li>
<li>There are different kinds of tensors</li>
<li>scalar: a single number of <em>0 dimensions</em></li>
<li>vector: a number with a direction of <em>1 dimension</em></li>
<li>matrix: a <em>2-dimensional</em> array of numbers</li>
<li>tensor: a <em>n-dimensional</em> array of numbers</li>
<li>Random tensors: important because neural networks take in tensors full of <em>random numbers</em> and then adjust those numbers via tensor operations (addition, subtraction, simple, element and matrix multiplication, division) to <strong>better represent</strong> data</li>
</ol>
<h3>Quickstart</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># ----- QUICKSTART -----</span>
    <span class="c1"># %%time =&gt; CPU time and Wall time for the execution of a given Jupyter notebook cell</span>
    <span class="c1"># torch.__version__ =&gt; current PyTorch version</span>
    <span class="c1"># torch.tensor() =&gt; initialises a tensor object literal, and can receive additional arguments</span>
        <span class="c1"># dtype =&gt; specifies the datatype of each element of the tensor</span>
            <span class="c1"># None</span>
            <span class="c1"># .bool =&gt; True, False</span>
            <span class="c1"># .float16</span>
            <span class="c1"># .float32 (assigned by default)</span>
            <span class="c1"># .float64</span>
            <span class="c1"># .complex32</span>
            <span class="c1"># .complex64</span>
            <span class="c1"># .complex128</span>
            <span class="c1"># .int8</span>
            <span class="c1"># .int16</span>
            <span class="c1"># .int32</span>
            <span class="c1"># .int64</span>
            <span class="c1"># .uint8</span>
            <span class="c1"># .uint16</span>
            <span class="c1"># .uint32</span>
            <span class="c1"># .uint64</span>
            <span class="c1"># .quint8</span>
            <span class="c1"># .qint8</span>
            <span class="c1"># .qint32</span>
            <span class="c1"># .quint4x2</span>
            <span class="c1"># .float8_e4m3fn</span>
            <span class="c1"># .float8_e5m2</span>
        <span class="c1"># device =&gt; specifies the device each tensor lives on</span>
            <span class="c1"># cpu</span>
            <span class="c1"># cuda</span>
            <span class="c1"># mps</span>
            <span class="c1"># xpu</span>
            <span class="c1"># xla</span>
            <span class="c1"># meta</span>
        <span class="c1"># requires_grad =&gt; specifies whether PyTorch should track the gradient of a tensor when it undergoes numerical calculations</span>
            <span class="c1"># True</span>
            <span class="c1"># False</span>
    <span class="c1"># torch.rand() =&gt; initialises a random tensor object of the specified torch.Size()</span>
    <span class="c1"># torch.zeros() =&gt; initialises a tensor of all zeros of the specified torch.Size(), most commonly used to create a mask</span>
    <span class="c1"># torch.ones() =&gt; initialises a tensor of all ones of the specified torch.Size(), most commonly used to create a mask</span>
    <span class="c1"># torch.zeros_like =&gt; initialises a tensor of all zeros of the torch.Size() from another specified tensor</span>
    <span class="c1"># torch.ones_like =&gt; initialises a tensor of all ones of the torch.Size() from another specified tensor</span>
    <span class="c1"># torch.arange(start, end, step) # initialises a tensor object literal from a range created from the specified start, end and step</span>
    <span class="c1"># .item() =&gt; called on a tensor object, which is then returned as a value literal (integer, list literal etc.)</span>
    <span class="c1"># .ndim =&gt; called on a tensor object to return the number of dimensions a given tensor has</span>
        <span class="c1"># observe that the rule of thumb is one dimension is added for every degree of [] square bracket nesting within a tensor object</span>
    <span class="c1"># .shape =&gt; recursive call on a tensor object to return the number of list elements within a given tensor</span>
    <span class="c1"># .dtype =&gt; method that returns the datatype of the specified variable it is called upon, PyTorch assigns the default datatype of .float32 if unspecified</span>
    <span class="c1"># .device =&gt; method that returns the current device of a given tensor object</span>

<span class="c1"># --- DATA SCIENCE PACKAGES TO IMPORT ---</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span> 
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">p</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="c1"># display the current PyTorch version</span>

<span class="c1"># --- USER-DEFINED TENSORS ---</span>

<span class="n">scalar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span> <span class="c1"># initialise a scalar tensor object</span>
<span class="n">scalar</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># returns 7</span>
<span class="n">scalar</span><span class="o">.</span><span class="n">ndim</span> <span class="c1"># returns 0 dimensions</span>
<span class="n">scalar</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([]) to indicate that there are no list elements</span>

<span class="n">vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span> <span class="c1"># intialise a vector tensor object</span>
<span class="n">vector</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># returns [7, 7]</span>
<span class="n">vector</span><span class="o">.</span><span class="n">ndim</span> <span class="c1"># returns 1 dimension</span>
<span class="n">vector</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size(2) to indicate 2 elements</span>

<span class="c1"># - NOTE -</span>
    <span class="c1"># by convention, scalar and vector variables are declared in lowercase while matrix and tensor variables are declared in UPPERCASE</span>

<span class="n">MATRIX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> 
    <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">MATRIX</span><span class="o">.</span><span class="n">ndim</span><span class="p">()</span> <span class="c1"># returns 2 dimensions</span>
<span class="n">MATRIX</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([2, 2]) to indicate 2 list elements each containing 2 elements</span>

<span class="n">TENSOR</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]]</span>
<span class="p">)</span>
<span class="n">TENSOR</span><span class="o">.</span><span class="n">ndim</span> <span class="c1"># returns 3 dimensions</span>
<span class="n">TENSOR</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([1, 3, 3]) to indicate 1 list element which contains 3 list elements which contain 3 elements each</span>

<span class="n">WATERMELON</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]]]</span>
<span class="p">)</span>
<span class="n">WATERMELON</span><span class="o">.</span><span class="n">ndim</span> <span class="c1"># returns 4 dimensions</span>
<span class="n">WATERMELON</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([1, 1, 6, 2]) to indicate 1 list element that contains 1 list element that contains 6 list elements which then contains 2 elements each</span>

<span class="c1"># --- RANDOM TENSOR ---</span>

<span class="n">RANDOM_TENSOR</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="c1"># initialises a random tensor of torch.Size([3, 4])</span>
<span class="n">RANDOM_TENSOR</span><span class="o">.</span><span class="n">ndim</span> <span class="c1"># returns 2 dimensions</span>

<span class="n">RANDOM_IMAGE_SIZE_TENSOR</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># initialises a random tensor with a similar shape to an image tensor, specifying the height, width and color channel</span>
<span class="n">RANDOM_IMAGE_SIZE_TENSOR</span><span class="o">.</span><span class="n">ndim</span> <span class="c1"># returns 3 dimensions as we specified above</span>
<span class="n">RANDOM_IMAGE_SIZE_TENSOR</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([224, 224, 3]) as we specified above</span>

<span class="c1"># --- TENSOR OF ALL 0s ---</span>

<span class="n">ZERO_TENSOR</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="c1"># initialises a tensor of all zeros of the torch.Size([3, 4])</span>

<span class="c1"># --- TENSOR OF ALL 1s ---</span>

<span class="n">ONE_TENSOR</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="c1"># initialises a tensor of all ones of the torch.Size([3, 4])</span>

<span class="c1"># --- RANGE TENSOR ---</span>

<span class="n">zero_to_nine</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># initialises the tensor object literal tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
<span class="n">two_to_eleven</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># initialises the tensor object literal tensor([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])</span>

<span class="c1"># --- TENSORS LIKE ---</span>

<span class="n">ten_zeroes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">zero_to_nine</span><span class="p">)</span> <span class="c1"># initialises a zero tensor of the same shape as the specified input tensor</span>
<span class="n">ten_ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">zero_to_nine</span><span class="p">)</span> <span class="c1"># initialises a one tensor of the same shape as the specified input tensor</span>
</code></pre></div>

<h3>Tensor operations</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># ----- TENSOR OPERATIONS -----</span>

<span class="c1"># --- ARITHMETIC METHODS ---</span>
    <span class="c1"># + =&gt; addition applied to each element of the tensor</span>
    <span class="c1"># - =&gt; subtraction applied to each element of the tensor</span>
    <span class="c1"># * =&gt; simple multiplication of a matrix against a scalar number</span>
    <span class="c1"># / =&gt; division applied to each element of the tensor</span>
    <span class="c1"># * =&gt; ELEMENT multiplication of two matrices, where each element of a matrix is multipled against its corresponding element in the other matrix</span>
    <span class="c1"># torch.matmul() =&gt; MATRIX multiplication that finds the DOT PRODUCT of two specified matrices by multiplying them together</span>
        <span class="c1"># observe that matrix multiplication must satisfy the following 2 rules</span>
            <span class="c1"># 1. inner dimensions of the two matrices must match</span>
                <span class="c1"># torch.matmul(torch.rand(2, 3), torch.rand(2, 3)) WON&#39;T work</span>
                <span class="c1"># torch.matmul(torch.rand(2, 3), torch.rand(3, 2)) WILL work</span>
                <span class="c1"># torch.matmul(torch.rand(3, 2), torch.rand(2, 3)) WILL work</span>
            <span class="c1"># 2. result matrix must have the shape of the outer dimensions</span>
                <span class="c1"># torch.matmul(torch.rand(2, 3), torch.rand(3, 2)) results in torch.Size([2, 2]) so this WILL work</span>
                <span class="c1"># torch.matmul(torch.rand(3, 2), torch.rand(2, 3)) results in torch.Size([3, 3]) so this WILL work</span>

<span class="c1"># --- AGGREGATOR METHODS ---</span>
    <span class="c1"># torch.min() =&gt; finds the element with the minimum value in a given tensor </span>
    <span class="c1"># torch.max() =&gt; finds the element with the maximum value in a given tensor </span>
    <span class="c1"># torch.mean() =&gt; finds the average value of all elements within a given tensor, note the element datatype must be a floating or complex type</span>
    <span class="c1"># torch.sum() =&gt; finds the sum of all elements within a given tensor</span>
    <span class="c1"># torch.argmin() =&gt; finds the index of the element with the minimum value in a given tensor </span>
    <span class="c1"># torch.argmax() =&gt; finds the index of the element with the maximum value in a given tensor </span>

<span class="c1"># --- MANIPULATION METHODS ---</span>
    <span class="c1"># one of the most common issues relating to tensors arises due to shape and dimension, which is combated by</span>
        <span class="c1"># reshaping =&gt; reshaping an input tensor to a specified shape</span>
        <span class="c1"># view =&gt; returns a view of an input tensor in a specified shape while pointing to the same place in memory as the original tensor</span>
        <span class="c1"># stacking =&gt; combine multiple tensors together in a vertical (vstack) or horizontal (hstack) stack</span>
        <span class="c1"># squeeze =&gt; remove all 1 dimensions from a given tensor</span>
        <span class="c1"># unsqueeze =&gt; add a 1 dimension to a given tensor</span>
        <span class="c1"># permute =&gt; returns a view of an input tensor with its dimensions swapped in a certain way</span>
    <span class="c1"># the PyTorch methods are as follows</span>
        <span class="c1"># .T =&gt; method that tranposes the shape of the specified tensor by switching its dimensions (axis), particularly useful for when tensor shape errors occur</span>
        <span class="c1"># .reshape() =&gt; method that reshapes a specified tensor to the new provided dimensions, note that the total corresponding number of elements must stay the same across a reshape</span>
        <span class="c1"># .view() =&gt; method that merely displays an existing tensor differenly according to the new provided dimensions whilst pointing to the original tensor&#39;s memory address (which means changing the new tensor variable assigned to a view changes the value of the original tensor being viewed)</span>
        <span class="c1"># torch.stack() =&gt; method that stacks multiple provided tensors together, with an optional dim argument that further allows augmentation of the desired number of dimensions within the new tensor</span>
            <span class="c1"># torch.vstack</span>
            <span class="c1"># torch.hstack</span>
        <span class="c1"># torch.squeeze() =&gt; method that removes all SINGLE dimensions from a given tensor</span>
        <span class="c1"># torch.unsqueeze() =&gt; method that adds a SINGLE dimension to a given tensor, with a dim argument that further specifies which dimension to add the single dimension at</span>
        <span class="c1"># torch.permute() =&gt; method that rearranges the dimensions of a given tensor to a new specified order and returns a VIEW of that new tensor (which means changing the new tensor variable assigned to a permute changes the value of the original tensor being permuted)</span>

<span class="c1"># --- SELECTION METHODS ---</span>
    <span class="c1"># [] =&gt; indexing in PyTorch is similar to indexing in Python and NumPy, where list values are zero-indexed and can have nested calls</span>
    <span class="c1"># : =&gt; specifies to select ALL of a given target dimension</span>

<span class="c1"># - NOTE -</span>
    <span class="c1"># recall that we have to reassign the result of a tensor operation to a variable for the value to be stored, similar to anywhere else in Python and most other programming languages really</span>
    <span class="c1"># the examples below are selected samples of the above methods and are not comprehensive, more detailed use cases can be found in PyTorch&#39;s documentation</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># intialisation of tensor object literals</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> 
<span class="n">another_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">yet_another_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># initialises the tensor object literal tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
<span class="n">final_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># initialises the tensor object literal tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]) </span>

<span class="n">tensor</span> <span class="o">+</span> <span class="mi">10</span> <span class="c1"># addition that evaluates to the tensor object literal tensor([101, 102, 103])</span>
<span class="n">tensor</span> <span class="o">-</span> <span class="mi">10</span> <span class="c1"># subtraction that evaluates to the tensor object literal tensor([-9, -8, -7])</span>
<span class="n">tensor</span> <span class="o">*</span> <span class="mi">10</span> <span class="c1"># simple multiplication evaluates to the tensor object literal tensor([10, 20, 30])</span>
<span class="n">tensor</span> <span class="o">/</span> <span class="mi">10</span> <span class="c1"># divison evaluates to the tensor object literal tensor([0.1, 0.2, 0.3])</span>

<span class="n">tensor</span> <span class="o">*</span> <span class="n">tensor</span> <span class="c1"># element multiplication evaluates to the tensor object literal tensor([1, 4, 9])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span> <span class="c1"># matrix multiplication evalutes to the dot product value torch(14)</span>

<span class="n">another_tensor</span><span class="o">.</span><span class="n">T</span> <span class="c1"># transpose operation that evaluates to the tensor object literal tensor([[7, 9, 11], [8, 10, 12]])</span>
<span class="n">another_tensor</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># transpose operation means this will now return the torch.Size([2, 3])</span>

<span class="n">yet_another_tensor</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([9])</span>
<span class="n">reshaped_tensor</span> <span class="o">=</span> <span class="n">yet_another_tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">reshaped_tensor</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([9, 1])</span>

<span class="n">view_tensor</span> <span class="o">=</span> <span class="n">yet_another_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">view_tensor</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># returns torch.Size([9, 1]), but note that modifying view_tensor will also modify the value of yet_another_tensor</span>

<span class="n">stack_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">yet_another_tensor</span><span class="p">,</span> <span class="n">yet_another_tensor</span><span class="p">,</span> <span class="n">yet_another_tensor</span><span class="p">,</span> <span class="n">yet_another_tensor</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># restacks the tensor according to dimension 0</span>
<span class="n">stack_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">yet_another_tensor</span><span class="p">,</span> <span class="n">yet_another_tensor</span><span class="p">,</span> <span class="n">yet_another_tensor</span><span class="p">,</span> <span class="n">yet_another_tensor</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># restacks the tensor according to dimension 1</span>

<span class="n">final_tensor</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># unmodified tensor will return torch.Size([1, 3, 3])</span>
<span class="n">final_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># evaluates to tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span>
<span class="n">final_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># evaluates to tensor([1, 2, 3])</span>
<span class="n">final_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># evaluates to tensor(1)</span>
</code></pre></div>

<h3>PyTorch and NumPy</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># ----- NUMPY -----</span>
    <span class="c1"># torch.from_numpy() =&gt; receives NumPy data and converts it to a PyTorch tensor</span>
    <span class="c1"># torch.Tensor.numpy() =&gt; receives a PyTorch tensor and converts it to NumPy data</span>
    <span class="c1"># observe that NumPy&#39;s default datatype is float64 while PyTorch&#39;s default datatype is float32</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>

<span class="n">array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span> <span class="c1"># initialise a NumPy array</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">)</span> <span class="c1"># convert that NumPy array to a PyTorch tensor</span>
<span class="n">back_to_array</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="c1"># converting that PyTorch tensor back to a NumPy array</span>
</code></pre></div>

<h3>Reproducibility</h3>
<p>Introduce a <strong>random seed</strong> to flavour the randomness of <code>torch.rand()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ----- REPRODUCIBILITY -----</span>
    <span class="c1"># torch.manual_seed() =&gt; sets the provided value as the seed for generating the next random tensor value to ensuring reproducibility</span>
        <span class="c1"># note that this method must be called EVERY TIME we want to invoke the torch.rand() method to reassign the user-defined seed value</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span> <span class="c1"># assign a seed value</span>
<span class="n">random_tensor_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span> <span class="c1"># assign a seed value</span>
<span class="n">random_tensor_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">random_tensor_1</span> <span class="o">==</span> <span class="n">random_tensor_2</span> <span class="c1"># this evaluates to True</span>
</code></pre></div>

<h2>Doing actual things with Tensors</h2>
<p><em>"Enough yapping, I want to build something."</em></p>
<h3>Encode an Image to a Tensor</h3>
<p>The model will be trained on the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>.  </p>
<ol>
<li>Split the image into its RGB <em>(red green blue)</em> color channels.</li>
<li>Represent that as a tensor with the shape <em>(<code>color_channels</code>, <code>image_height</code>, <code>image_width</code>)</em>.</li>
<li>Train the model using a basic convolutional neural network (CNN).</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># ----- PREPARATION WORK -----</span>

<span class="c1"># --- required imports ---</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="c1"># --- preprocess the CIFAR-10 image dataset ---</span>
    <span class="c1"># defines transformations for the training set</span>
    <span class="c1"># flips the images randomly for additional fed data</span>
    <span class="c1"># load the actual training set and test set</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>  <span class="c1"># randomly flip the image horizontally</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>  <span class="c1"># crop the image to 32x32 with padding</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>  <span class="c1"># convert the image to a PyTorch tensor</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.4914</span><span class="p">,</span> <span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.4465</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2023</span><span class="p">,</span> <span class="mf">0.1994</span><span class="p">,</span> <span class="mf">0.2010</span><span class="p">)),</span>  <span class="c1"># normalize the image</span>
<span class="p">])</span>

<span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>  <span class="c1"># download and transform training data</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># create data loader for training</span>
<span class="n">testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>  <span class="c1"># download and transform test data</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># create data loader for testing</span>

<span class="n">classes</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;plane&#39;</span><span class="p">,</span> <span class="s1">&#39;car&#39;</span><span class="p">,</span> <span class="s1">&#39;bird&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;deer&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;frog&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span> <span class="s1">&#39;ship&#39;</span><span class="p">,</span> <span class="s1">&#39;truck&#39;</span><span class="p">)</span>  <span class="c1"># class labels</span>

<span class="c1"># --- specifying CNN architecture ---</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  <span class="c1"># initialize the parent class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># first convolutional layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># second convolutional layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># max pooling layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># first fully connected layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># second fully connected layer</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c1"># apply first conv layer + ReLU + pooling</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c1"># apply second conv layer + ReLU + pooling</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">*</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># flatten the tensor</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># apply first FC layer + ReLU</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># apply second FC layer</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># ----- EXECUTION CODE -----</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">SimpleCNN</span><span class="p">()</span>  <span class="c1"># instantiate an instance of the CNN model</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># define the loss function</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>  <span class="c1"># define the optimizer</span>

<span class="c1"># --- training loop ---</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># initialize loss</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>  <span class="c1"># iterate through batches</span>

        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>  <span class="c1"># get the inputs and labels</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># zero the parameter gradients</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># forward pass</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  <span class="c1"># compute loss</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># backward pass</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update weights</span>

        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># accumulate loss</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>  <span class="c1"># print every 100 mini-batches</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">] loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>  <span class="c1"># print loss</span>
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># reset running loss</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;we have finished training the model hooray!&#39;</span><span class="p">)</span>  <span class="c1"># indicate end of training loop</span>

<span class="c1"># --- evaluate accuracy of the model ---</span>

<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># turn off gradient computation</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>  <span class="c1"># iterate through test data</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>  <span class="c1"># get the images and labels</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>  <span class="c1"># forward pass</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># get the predicted labels</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># update total count</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># update correct count</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;accuracy of the network on the 10000 test images: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total</span><span class="si">}</span><span class="s1"> %&#39;</span><span class="p">)</span>  <span class="c1"># model accuracy</span>
</code></pre></div>

<h2>More on</h2>
<h3>Core resources</h3>
<ul>
<li><a href="https://pytorch.org/get-started/locally/">install pytorch locally</a></li>
<li><a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/SETUP.md">getting setup with pytorch</a></li>
<li><a href="https://pytorch.org/">pytorch.org</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">pytorch documentation</a></li>
<li><a href="https://paperswithcode.com/trends">papers with code trends</a></li>
<li><a href="https://www.reddit.com/r/MLQuestions/comments/112sege/pytorch_vs_tensorflow/">why pytorch over tensorflow</a></li>
<li><a href="https://www.learnpytorch.io/">zero to mastery: learn pytorch for deep learning</a></li>
</ul>
<h3>Additional resources</h3>
<ul>
<li><a href="https://colab.google/">colab.google</a></li>
<li><a href="https://aws.amazon.com/">aws.amazon</a></li>
<li><a href="https://www.tensorflow.org/">tensorflow.org</a></li>
<li><a href="https://keras.io/">keras.io</a></li>
<li><a href="https://github.com/meta-llama/llama">llama2</a></li>
</ul>
<h3>Prerequisite knowledge</h3>
<ul>
<li><a href="https://learnxinyminutes.com/docs/python/">learn python in y minutes</a></li>
<li><a href="https://numpy.org/">numpy</a></li>
<li><a href="https://pandas.pydata.org/">pandas</a></li>
<li><a href="https://matplotlib.org/">matplotlib</a></li>
<li><a href="https://www.mathsisfun.com/algebra/matrix-multiplying.html">how to find dot product</a></li>
<li><a href="https://aws.amazon.com/what-is-cloud-computing/">what is cloud computing</a></li>
</ul>
      </section>

    </article>

    <footer>
      <p> 2023-<span id="current-year"></span> Gabriel Ong. All rights reserved.</p>
    </footer>
  </main>

  <div class="wrapper"></div>
</body>
</html>
