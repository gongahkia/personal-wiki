<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>LargeLanguageModels</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body>
<h1 id="large-language-models"><code>Large Language Models</code></h1>
<p>Also known as LLMs.</p>
<h2 id="introduction">Introduction</h2>
<p>LLMs are typically built upon the following infrastructure.</p>
<ol type="1">
<li><a href="https://huggingface.co/docs/text-generation-inference/en/index">Text Generation Interface (TGI)</a>: framework that features optimized transformer code, quantization, accelerated weight loading and logits warping</li>
<li><a href="https://github.com/huggingface/transformers">Hugging Face Transformers (HG)</a>: open-source library that provides many pre-trained models for NLP and other custom tasks</li>
<li><a href="https://github.com/vllm-project/vllm">Versatile Large Language Model (vLLM)</a>: framework that features efficient memory management with paged attention, optimized CUDA kernels, decoding algorithms and high-performance serving throughput <em>(outperforming HF and TGI by significant margins)</em></li>
</ol>
<h2 id="quickstart">Quickstart</h2>
<p>You can <a href="#train-your-own-model">train your own model</a>, or <a href="#use-a-prebuilt-model">use existing ones</a>.</p>
<h2 id="train-your-own-model">Train your own model</h2>
<p>First, learn <a href="https://learnxinyminutes.com/docs/python/">Python</a>.</p>
<p>Optionally, learn <a href="https://learnxinyminutes.com/docs/r/">R</a>, <a href="https://learnxinyminutes.com/docs/julia/">Julia</a>, <a href="https://learnxinyminutes.com/docs/c++/">C++</a>, <a href="https://learnxinyminutes.com/docs/scala/">Scala</a> and <a href="https://learnxinyminutes.com/docs/go/">Go</a>.</p>
<p>Then choose a library from below.</p>
<h3 id="libraries">Libraries</h3>
<h4 id="python">Python</h4>
<ul>
<li><a href="https://pytorch.org/">PyTorch</a></li>
<li><a href="https://www.tensorflow.org/">TensorFlow</a></li>
<li><a href="https://huggingface.co/docs/transformers/en/index">Hugging Face Transformers</a></li>
<li><a href="https://www.nltk.org/">Natural Language Toolkit</a></li>
<li><a href="https://spacy.io/">spaCy</a></li>
<li><a href="https://github.com/PygmalionAI/aphrodite-engine">Aphrodite Engine</a></li>
</ul>
<h4 id="r">R</h4>
<ul>
<li><a href="https://www.tidyverse.org/packages/">Tidyverse</a></li>
<li><a href="https://topepo.github.io/caret/">caret</a></li>
<li><a href="https://cran.r-project.org/web/packages/tm/index.html">tm</a></li>
</ul>
<h4 id="julia">Julia</h4>
<ul>
<li><a href="https://fluxml.ai/Flux.jl/stable/">Flux</a></li>
<li><a href="https://juliaai.github.io/MLJ.jl/dev/">MLJ</a></li>
</ul>
<h4 id="c">C++</h4>
<ul>
<li><a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a></li>
<li><a href="http://dlib.net/">dlib</a></li>
<li><a href="https://www.tensorflow.org/api_docs/cc">TensorFlow core</a></li>
</ul>
<h4 id="scala">Scala</h4>
<ul>
<li><a href="https://spark.apache.org/mllib/">MLlib</a></li>
<li><a href="https://github.com/scalanlp/breeze">Breeze</a></li>
<li><a href="https://deeplearning4j.konduit.ai/">DeepLearning4j</a></li>
<li><a href="https://www.weka.io/">Weka</a></li>
</ul>
<h4 id="go">Go</h4>
<ul>
<li><a href="https://github.com/knights-analytics/hugot">Hugot</a></li>
<li><a href="https://gorgonia.org/">Gorgonia</a></li>
</ul>
<h4 id="language-agnostic">Language-agnostic</h4>
<ul>
<li><a href="https://ollama.com/">Ollama</a></li>
<li><a href="https://github.com/theroyallab/tabbyAPI">TabbyAPI</a></li>
</ul>
<h2 id="use-a-prebuilt-model">Use a prebuilt model</h2>
<p>In 2024, there are many existing LLM implementations to choose from. The more prominent ones have been listed below.</p>
<ul>
<li><a href="https://openai.com/index/gpt-4/">GPT-4</a>
<ul>
<li>OpenAI-developed</li>
<li>excellent at generating human-like text</li>
</ul></li>
<li><a href="https://research.google/pubs/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/">Bidirectional Encoder Representations from Transformers (BERT)</a>
<ul>
<li>Google-developed</li>
<li>transformer-based model for a variety of NLP tasks</li>
</ul></li>
<li><a href="https://huggingface.co/docs/accelerate/en/usage_guides/megatron_lm">Megatron-LM</a>
<ul>
<li>NVIDIA-developed</li>
<li>scalable LLM framework for training and deploying models</li>
</ul></li>
<li><a href="https://llama.meta.com/">Llama</a>
<ul>
<li>Meta-developed</li>
<li>family of large language models for NLP and text generation</li>
</ul></li>
<li><a href="https://ai.meta.com/tools/fairseq/">Fairseq</a>
<ul>
<li>Meta-developed <em>(Facebook AI Research (FAIR))</em></li>
<li>sequence-to-sequence learning toolkit for training and deploying models</li>
<li>used for translation, summarization and language modeling</li>
</ul></li>
<li><a href="https://allenai.org/allennlp">AllenNLP</a>
<ul>
<li>Allen Institute for AI-developed</li>
<li>open-source library built on PyTorch</li>
<li>used for NLP research and deploying NLP-focused models</li>
</ul></li>
<li><a href="https://huggingface.co/docs/transformers/en/model_doc/t5">Text-To-Text Transfer Transformer (T5)</a>
<ul>
<li>Google-developed</li>
<li>highly versatile model that frames all NLP tasks as converting text to text</li>
</ul></li>
<li><a href="http://research.baidu.com/Blog/index-view?id=183">Enhanced Representation through Knowledge Integration (ERNIE)</a>
<ul>
<li>Baidu-developed</li>
<li>incorporates knowledge graphs into the pre-training process for enhanced language comprehension</li>
</ul></li>
<li><a href="https://paperswithcode.com/method/ulmfit">Universal Language Model Fine-tuning (ULMFiT)</a>
<ul>
<li>fast.ai-developed</li>
<li>technique for fine-tuning pre-trained LLMs on downstream tasks</li>
</ul></li>
<li><a href="https://huggingface.co/bigcode/starcoder">StarCoder</a>
<ul>
<li>BigCode-developed</li>
<li>optimized for code generation and completion</li>
</ul></li>
<li><a href="https://huggingface.co/bigscience/bloom">BLOOM</a>
<ul>
<li>BigScience-developed</li>
<li>multilingual language model trained on many languages and tasks</li>
</ul></li>
<li><a href="https://huggingface.co/docs/transformers/en/model_doc/gpt_neox">GPT-NeoX</a>
<ul>
<li>EleutherAI-developed</li>
<li>designed to replicate GPT-3 architecture for various NLP tasks</li>
</ul></li>
<li><a href="https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling">Pythia</a>
<ul>
<li>EleutherAI-developed</li>
<li>family of models for large-scale NLP tasks</li>
</ul></li>
<li><a href="https://huggingface.co/OpenAssistant">OpenAssistant</a>
<ul>
<li>LAION-developed</li>
<li>conversational assistant for interactive AI dialogue capabilities</li>
</ul></li>
<li><a href="https://huggingface.co/databricks/dolly-v2-12b">Dolly V2</a>
<ul>
<li>Databricks-developed</li>
<li>high-performance commercial model for instruction-following tasks</li>
</ul></li>
<li><a href="https://github.com/Stability-AI/StableLM">StableLM</a>
<ul>
<li>Stability AI-developed</li>
<li>robust model for NLP tasks</li>
</ul></li>
<li><a href="https://localai.io/">LocalAI</a>
<ul>
<li>LocalAI-developed</li>
<li>facilitates local deployment of existing LLMs without relying on cloud-based services</li>
</ul></li>
</ul>
<h2 id="more-on">More on</h2>
<ul>
<li><a href="https://github.com/eugeneyan/open-llms">Open LLMs</a> Github repository</li>
<li><a href="https://github.com/Hannibal046/Awesome-LLM">Awesome-LLM</a> Github repository</li>
<li><a href="https://medium.com/@rohit.k/tgi-vs-vllm-making-informed-choices-for-llm-deployment-37c56d7ff705">TGI vs vLLM</a> by Rohit Kewalramani</li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1cb8i7f/which_is_faster_vllm_tgi_or_tensorrt/">Which is faster, vLLM, TGI or TensorRT?</a> Reddit post</li>
</ul>
</body>
</html>
