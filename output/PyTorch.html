<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>PyTorch</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<h1 id="pytorch"><code>PyTorch</code></h1>
<p>Deep learning in Python.</p>
<h2 id="introduction">Introduction</h2>
<h3 id="definitions">Definitions</h3>
<ol type="1">
<li>Deep learning: program infers relationships between user-defined <em>inputs</em> and <em>outputs</em> (supervised learning)</li>
<li>Deep learning is good for
<ul>
<li>problems with long lists of rules that would be difficult to hardcode</li>
<li>continually changing environments</li>
<li>large unstructured datasets with unclear patterns</li>
</ul></li>
<li>Deep learning is mostly achieved through a multilayered neural network <em>(hence deep)</em></li>
<li>Neural networks work by
<ol type="1">
<li><strong>Input layer</strong>: <em>inputs</em> are numerically encoded into multi-dimensional vectors</li>
<li><strong>Hidden layer</strong>: vectors are processed by the many hidden layers, program finds patterns in the vectors</li>
<li><strong>Output layer</strong>: <em>output</em> is returned in the form of multi-dimensional vectors</li>
</ol></li>
<li>There are multiple types of neural networks
<ul>
<li>convolutional neural network (images)</li>
<li>transformer (NLP)</li>
</ul></li>
<li>There are a few kinds of learning
<ul>
<li>supervised learning: BOTH <em>inputs</em> and <em>outputs</em> are specified for the program</li>
<li>unsupervised / self-supervised learning: ONLY <em>input</em> is specified for the program</li>
<li>transfer learning: one program’s <em>output</em> is FED to another program as <em>input</em></li>
<li>reinforcement learning: program is REWARDED for <em>ideal</em> behaviour and discouraged from <em>unideal</em> behaviour</li>
</ul></li>
<li>General workflow of building a deep learning program is
<ol type="1">
<li>convert raw data to <em>input tensors</em></li>
<li>build a model
<ul>
<li>pick a loss function and optimizer</li>
<li>create the training loop</li>
</ul></li>
<li>tweak the model to fit the <em>data</em> and make a prediction with <em>output tensors</em></li>
<li>evaluate the model</li>
<li>improve the model through iterative experimentation</li>
<li>save and reload</li>
</ol></li>
<li>Tensor: any numerical representation of data <em>(most commonly multi-dimensional vectors)</em></li>
<li>There are different kinds of tensors
<ol type="1">
<li>scalar: a single number of <em>0 dimensions</em></li>
<li>vector: a number with a direction of <em>1 dimension</em></li>
<li>matrix: a <em>2-dimensional</em> array of numbers</li>
<li>tensor: a <em>n-dimensional</em> array of numbers</li>
</ol></li>
<li>Random tensors: important because neural networks take in tensors full of <em>random numbers</em> and then adjust those numbers via tensor operations (addition, subtraction, simple, element and matrix multiplication, division) to <strong>better represent</strong> data</li>
</ol>
<h3 id="quickstart">Quickstart</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co"># ----- QUICKSTART -----</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    <span class="co"># %%time =&gt; CPU time and Wall time for the execution of a given Jupyter notebook cell</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>    <span class="co"># torch.__version__ =&gt; current PyTorch version</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    <span class="co"># torch.tensor() =&gt; initialises a tensor object literal, and can receive additional arguments</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>        <span class="co"># dtype =&gt; specifies the datatype of each element of the tensor</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>            <span class="co"># None</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>            <span class="co"># .bool =&gt; True, False</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a>            <span class="co"># .float16</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a>            <span class="co"># .float32 (assigned by default)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a>            <span class="co"># .float64</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>            <span class="co"># .complex32</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>            <span class="co"># .complex64</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a>            <span class="co"># .complex128</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true"></a>            <span class="co"># .int8</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true"></a>            <span class="co"># .int16</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true"></a>            <span class="co"># .int32</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true"></a>            <span class="co"># .int64</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true"></a>            <span class="co"># .uint8</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true"></a>            <span class="co"># .uint16</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true"></a>            <span class="co"># .uint32</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true"></a>            <span class="co"># .uint64</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true"></a>            <span class="co"># .quint8</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true"></a>            <span class="co"># .qint8</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true"></a>            <span class="co"># .qint32</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true"></a>            <span class="co"># .quint4x2</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true"></a>            <span class="co"># .float8_e4m3fn</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true"></a>            <span class="co"># .float8_e5m2</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true"></a>        <span class="co"># device =&gt; specifies the device each tensor lives on</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true"></a>            <span class="co"># cpu</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true"></a>            <span class="co"># cuda</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true"></a>            <span class="co"># mps</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true"></a>            <span class="co"># xpu</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true"></a>            <span class="co"># xla</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true"></a>            <span class="co"># meta</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true"></a>        <span class="co"># requires_grad =&gt; specifies whether PyTorch should track the gradient of a tensor when it undergoes numerical calculations</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true"></a>            <span class="co"># True</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true"></a>            <span class="co"># False</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true"></a>    <span class="co"># torch.rand() =&gt; initialises a random tensor object of the specified torch.Size()</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true"></a>    <span class="co"># torch.zeros() =&gt; initialises a tensor of all zeros of the specified torch.Size(), most commonly used to create a mask</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true"></a>    <span class="co"># torch.ones() =&gt; initialises a tensor of all ones of the specified torch.Size(), most commonly used to create a mask</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true"></a>    <span class="co"># torch.zeros_like =&gt; initialises a tensor of all zeros of the torch.Size() from another specified tensor</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true"></a>    <span class="co"># torch.ones_like =&gt; initialises a tensor of all ones of the torch.Size() from another specified tensor</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true"></a>    <span class="co"># torch.arange(start, end, step) # initialises a tensor object literal from a range created from the specified start, end and step</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true"></a>    <span class="co"># .item() =&gt; called on a tensor object, which is then returned as a value literal (integer, list literal etc.)</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true"></a>    <span class="co"># .ndim =&gt; called on a tensor object to return the number of dimensions a given tensor has</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true"></a>        <span class="co"># observe that the rule of thumb is one dimension is added for every degree of [] square bracket nesting within a tensor object</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true"></a>    <span class="co"># .shape =&gt; recursive call on a tensor object to return the number of list elements within a given tensor</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true"></a>    <span class="co"># .dtype =&gt; method that returns the datatype of the specified variable it is called upon, PyTorch assigns the default datatype of .float32 if unspecified</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true"></a>    <span class="co"># .device =&gt; method that returns the current device of a given tensor object</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true"></a><span class="co"># --- DATA SCIENCE PACKAGES TO IMPORT ---</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true"></a><span class="im">import</span> torch </span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true"></a><span class="im">import</span> pandas</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true"></a><span class="im">import</span> numpy</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> p</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true"></a><span class="bu">print</span>(torch.__version__) <span class="co"># display the current PyTorch version</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true"></a><span class="co"># --- USER-DEFINED TENSORS ---</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true"></a>scalar <span class="op">=</span> torch.tensor(<span class="dv">7</span>) <span class="co"># initialise a scalar tensor object</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true"></a>scalar.item() <span class="co"># returns 7</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true"></a>scalar.ndim <span class="co"># returns 0 dimensions</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true"></a>scalar.shape <span class="co"># returns torch.Size([]) to indicate that there are no list elements</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true"></a>vector <span class="op">=</span> torch.tensor([<span class="dv">7</span>, <span class="dv">7</span>]) <span class="co"># intialise a vector tensor object</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true"></a>vector.item() <span class="co"># returns [7, 7]</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true"></a>vector.ndim <span class="co"># returns 1 dimension</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true"></a>vector.shape <span class="co"># returns torch.Size(2) to indicate 2 elements</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true"></a><span class="co"># - </span><span class="al">NOTE</span><span class="co"> -</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true"></a>    <span class="co"># by convention, scalar and vector variables are declared in lowercase while matrix and tensor variables are declared in UPPERCASE</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true"></a>MATRIX <span class="op">=</span> torch.tensor(</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true"></a>    [[<span class="dv">7</span>, <span class="dv">8</span>], </span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true"></a>    [<span class="dv">9</span>, <span class="dv">10</span>]]</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true"></a>)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true"></a>MATRIX.ndim() <span class="co"># returns 2 dimensions</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true"></a>MATRIX.shape <span class="co"># returns torch.Size([2, 2]) to indicate 2 list elements each containing 2 elements</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true"></a>TENSOR <span class="op">=</span> torch.tensor(</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true"></a>    [[[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true"></a>    [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>],</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true"></a>    [<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>]]]</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true"></a>)</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true"></a>TENSOR.ndim <span class="co"># returns 3 dimensions</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true"></a>TENSOR.shape <span class="co"># returns torch.Size([1, 3, 3]) to indicate 1 list element which contains 3 list elements which contain 3 elements each</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true"></a>WATERMELON <span class="op">=</span> torch.tensor(</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true"></a>    [[[[<span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true"></a>    [<span class="dv">3</span>, <span class="dv">4</span>],</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true"></a>    [<span class="dv">5</span>, <span class="dv">6</span>],</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true"></a>    [<span class="dv">7</span>, <span class="dv">8</span>],</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true"></a>    [<span class="dv">9</span>, <span class="dv">10</span>],</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true"></a>    [<span class="dv">11</span>, <span class="dv">12</span>]]]]</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true"></a>)</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true"></a>WATERMELON.ndim <span class="co"># returns 4 dimensions</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true"></a>WATERMELON.shape <span class="co"># returns torch.Size([1, 1, 6, 2]) to indicate 1 list element that contains 1 list element that contains 6 list elements which then contains 2 elements each</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true"></a><span class="co"># --- RANDOM TENSOR ---</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true"></a>RANDOM_TENSOR <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">4</span>) <span class="co"># initialises a random tensor of torch.Size([3, 4])</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true"></a>RANDOM_TENSOR.ndim <span class="co"># returns 2 dimensions</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true"></a>RANDOM_IMAGE_SIZE_TENSOR <span class="op">=</span> torch.rand(size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>, <span class="dv">3</span>)) <span class="co"># initialises a random tensor with a similar shape to an image tensor, specifying the height, width and color channel</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true"></a>RANDOM_IMAGE_SIZE_TENSOR.ndim <span class="co"># returns 3 dimensions as we specified above</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true"></a>RANDOM_IMAGE_SIZE_TENSOR.shape <span class="co"># returns torch.Size([224, 224, 3]) as we specified above</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true"></a><span class="co"># --- TENSOR OF ALL 0s ---</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true"></a>ZERO_TENSOR <span class="op">=</span> torch.zeros(size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">4</span>)) <span class="co"># initialises a tensor of all zeros of the torch.Size([3, 4])</span></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true"></a><span class="co"># --- TENSOR OF ALL 1s ---</span></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true"></a>ONE_TENSOR <span class="op">=</span> torch.ones(size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">4</span>)) <span class="co"># initialises a tensor of all ones of the torch.Size([3, 4])</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true"></a><span class="co"># --- RANGE TENSOR ---</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true"></a>zero_to_nine <span class="op">=</span> torch.arange(<span class="dv">0</span>, <span class="dv">10</span>) <span class="co"># initialises the tensor object literal tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true"></a>two_to_eleven <span class="op">=</span> torch.arange(start<span class="op">=</span><span class="dv">2</span>, end<span class="op">=</span><span class="dv">11</span>, step<span class="op">=</span><span class="dv">1</span>) <span class="co"># initialises the tensor object literal tensor([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true"></a><span class="co"># --- TENSORS LIKE ---</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true"></a>ten_zeroes <span class="op">=</span> torch.zeros_like(<span class="bu">input</span><span class="op">=</span>zero_to_nine) <span class="co"># initialises a zero tensor of the same shape as the specified input tensor</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true"></a>ten_ones <span class="op">=</span> torch.ones_like(<span class="bu">input</span><span class="op">=</span>zero_to_nine) <span class="co"># initialises a one tensor of the same shape as the specified input tensor</span></span></code></pre></div>
<h3 id="tensor-operations">Tensor operations</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="co"># ----- TENSOR OPERATIONS -----</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="co"># --- ARITHMETIC METHODS ---</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>    <span class="co"># + =&gt; addition applied to each element of the tensor</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>    <span class="co"># - =&gt; subtraction applied to each element of the tensor</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>    <span class="co"># * =&gt; simple multiplication of a matrix against a scalar number</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>    <span class="co"># / =&gt; division applied to each element of the tensor</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>    <span class="co"># * =&gt; ELEMENT multiplication of two matrices, where each element of a matrix is multipled against its corresponding element in the other matrix</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>    <span class="co"># torch.matmul() =&gt; MATRIX multiplication that finds the DOT PRODUCT of two specified matrices by multiplying them together</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>        <span class="co"># observe that matrix multiplication must satisfy the following 2 rules</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a>            <span class="co"># 1. inner dimensions of the two matrices must match</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a>                <span class="co"># torch.matmul(torch.rand(2, 3), torch.rand(2, 3)) WON&#39;T work</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a>                <span class="co"># torch.matmul(torch.rand(2, 3), torch.rand(3, 2)) WILL work</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true"></a>                <span class="co"># torch.matmul(torch.rand(3, 2), torch.rand(2, 3)) WILL work</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true"></a>            <span class="co"># 2. result matrix must have the shape of the outer dimensions</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true"></a>                <span class="co"># torch.matmul(torch.rand(2, 3), torch.rand(3, 2)) results in torch.Size([2, 2]) so this WILL work</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true"></a>                <span class="co"># torch.matmul(torch.rand(3, 2), torch.rand(2, 3)) results in torch.Size([3, 3]) so this WILL work</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true"></a><span class="co"># --- AGGREGATOR METHODS ---</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true"></a>    <span class="co"># torch.min() =&gt; finds the element with the minimum value in a given tensor </span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true"></a>    <span class="co"># torch.max() =&gt; finds the element with the maximum value in a given tensor </span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true"></a>    <span class="co"># torch.mean() =&gt; finds the average value of all elements within a given tensor, note the element datatype must be a floating or complex type</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true"></a>    <span class="co"># torch.sum() =&gt; finds the sum of all elements within a given tensor</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true"></a>    <span class="co"># torch.argmin() =&gt; finds the index of the element with the minimum value in a given tensor </span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true"></a>    <span class="co"># torch.argmax() =&gt; finds the index of the element with the maximum value in a given tensor </span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true"></a><span class="co"># --- MANIPULATION METHODS ---</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true"></a>    <span class="co"># one of the most common issues relating to tensors arises due to shape and dimension, which is combated by</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true"></a>        <span class="co"># reshaping =&gt; reshaping an input tensor to a specified shape</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true"></a>        <span class="co"># view =&gt; returns a view of an input tensor in a specified shape while pointing to the same place in memory as the original tensor</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true"></a>        <span class="co"># stacking =&gt; combine multiple tensors together in a vertical (vstack) or horizontal (hstack) stack</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true"></a>        <span class="co"># squeeze =&gt; remove all 1 dimensions from a given tensor</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true"></a>        <span class="co"># unsqueeze =&gt; add a 1 dimension to a given tensor</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true"></a>        <span class="co"># permute =&gt; returns a view of an input tensor with its dimensions swapped in a certain way</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true"></a>    <span class="co"># the PyTorch methods are as follows</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true"></a>        <span class="co"># .T =&gt; method that tranposes the shape of the specified tensor by switching its dimensions (axis), particularly useful for when tensor shape errors occur</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true"></a>        <span class="co"># .reshape() =&gt; method that reshapes a specified tensor to the new provided dimensions, note that the total corresponding number of elements must stay the same across a reshape</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true"></a>        <span class="co"># .view() =&gt; method that merely displays an existing tensor differenly according to the new provided dimensions whilst pointing to the original tensor&#39;s memory address (which means changing the new tensor variable assigned to a view changes the value of the original tensor being viewed)</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true"></a>        <span class="co"># torch.stack() =&gt; method that stacks multiple provided tensors together, with an optional dim argument that further allows augmentation of the desired number of dimensions within the new tensor</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true"></a>            <span class="co"># torch.vstack</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true"></a>            <span class="co"># torch.hstack</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true"></a>        <span class="co"># torch.squeeze() =&gt; method that removes all SINGLE dimensions from a given tensor</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true"></a>        <span class="co"># torch.unsqueeze() =&gt; method that adds a SINGLE dimension to a given tensor, with a dim argument that further specifies which dimension to add the single dimension at</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true"></a>        <span class="co"># torch.permute() =&gt; method that rearranges the dimensions of a given tensor to a new specified order and returns a VIEW of that new tensor (which means changing the new tensor variable assigned to a permute changes the value of the original tensor being permuted)</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true"></a><span class="co"># --- SELECTION METHODS ---</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true"></a>    <span class="co"># [] =&gt; indexing in PyTorch is similar to indexing in Python and NumPy, where list values are zero-indexed and can have nested calls</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true"></a>    <span class="co"># : =&gt; specifies to select ALL of a given target dimension</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true"></a><span class="co"># - </span><span class="al">NOTE</span><span class="co"> -</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true"></a>    <span class="co"># recall that we have to reassign the result of a tensor operation to a variable for the value to be stored, similar to anywhere else in Python and most other programming languages really</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true"></a>    <span class="co"># the examples below are selected samples of the above methods and are not comprehensive, more detailed use cases can be found in PyTorch&#39;s documentation</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true"></a><span class="co"># intialisation of tensor object literals</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true"></a>tensor <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]) </span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true"></a>another_tensor <span class="op">=</span> torch.tensor(</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true"></a>    [[<span class="dv">7</span>, <span class="dv">8</span>],</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true"></a>    [<span class="dv">9</span>, <span class="dv">10</span>],</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true"></a>    [<span class="dv">11</span>, <span class="dv">12</span>]]</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true"></a>)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true"></a>yet_another_tensor <span class="op">=</span> torch.arange(<span class="dv">1</span>, <span class="dv">10</span>) <span class="co"># initialises the tensor object literal tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true"></a>final_tensor <span class="op">=</span> torch.arange(<span class="dv">1</span>, <span class="dv">10</span>).reshape(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>) <span class="co"># initialises the tensor object literal tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]) </span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true"></a>tensor <span class="op">+</span> <span class="dv">10</span> <span class="co"># addition that evaluates to the tensor object literal tensor([101, 102, 103])</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true"></a>tensor <span class="op">-</span> <span class="dv">10</span> <span class="co"># subtraction that evaluates to the tensor object literal tensor([-9, -8, -7])</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true"></a>tensor <span class="op">*</span> <span class="dv">10</span> <span class="co"># simple multiplication evaluates to the tensor object literal tensor([10, 20, 30])</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true"></a>tensor <span class="op">/</span> <span class="dv">10</span> <span class="co"># divison evaluates to the tensor object literal tensor([0.1, 0.2, 0.3])</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true"></a>tensor <span class="op">*</span> tensor <span class="co"># element multiplication evaluates to the tensor object literal tensor([1, 4, 9])</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true"></a>torch.matmul(tensor, tensor) <span class="co"># matrix multiplication evalutes to the dot product value torch(14)</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true"></a>another_tensor.T <span class="co"># transpose operation that evaluates to the tensor object literal tensor([[7, 9, 11], [8, 10, 12]])</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true"></a>another_tensor.T.shape <span class="co"># transpose operation means this will now return the torch.Size([2, 3])</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true"></a>yet_another_tensor.shape <span class="co"># returns torch.Size([9])</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true"></a>reshaped_tensor <span class="op">=</span> yet_another_tensor.reshape(<span class="dv">9</span>, <span class="dv">1</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true"></a>reshaped_tensor.shape <span class="co"># returns torch.Size([9, 1])</span></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true"></a>view_tensor <span class="op">=</span> yet_another_tensor.view(<span class="dv">9</span>, <span class="dv">1</span>)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true"></a>view_tensor.shape <span class="co"># returns torch.Size([9, 1]), but note that modifying view_tensor will also modify the value of yet_another_tensor</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true"></a>stack_tensor <span class="op">=</span> torch.stack([yet_another_tensor, yet_another_tensor, yet_another_tensor, yet_another_tensor], dim <span class="op">=</span> <span class="dv">0</span>) <span class="co"># restacks the tensor according to dimension 0</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true"></a>stack_tensor <span class="op">=</span> torch.stack([yet_another_tensor, yet_another_tensor, yet_another_tensor, yet_another_tensor], dim <span class="op">=</span> <span class="dv">1</span>) <span class="co"># restacks the tensor according to dimension 1</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true"></a>final_tensor.shape <span class="co"># unmodified tensor will return torch.Size([1, 3, 3])</span></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true"></a>final_tensor[<span class="dv">0</span>] <span class="co"># evaluates to tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true"></a>final_tensor[<span class="dv">0</span>][<span class="dv">0</span>] <span class="co"># evaluates to tensor([1, 2, 3])</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true"></a>final_tensor[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>] <span class="co"># evaluates to tensor(1)</span></span></code></pre></div>
<h3 id="pytorch-and-numpy">PyTorch and NumPy</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="co"># ----- NUMPY -----</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>    <span class="co"># torch.from_numpy() =&gt; receives NumPy data and converts it to a PyTorch tensor</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>    <span class="co"># torch.Tensor.numpy() =&gt; receives a PyTorch tensor and converts it to NumPy data</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>    <span class="co"># observe that NumPy&#39;s default datatype is float64 while PyTorch&#39;s default datatype is float32</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a><span class="im">import</span> numpy</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>array <span class="op">=</span> numpy.arange(<span class="fl">1.0</span>, <span class="fl">8.0</span>) <span class="co"># initialise a NumPy array</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a>tensor <span class="op">=</span> torch.from_numpy(array) <span class="co"># convert that NumPy array to a PyTorch tensor</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a>back_to_array <span class="op">=</span> torch.Tensor.numpy(tensor) <span class="co"># converting that PyTorch tensor back to a NumPy array</span></span></code></pre></div>
<h3 id="reproducibility">Reproducibility</h3>
<p>Introduce a <strong>random seed</strong> to flavour the randomness of <code>torch.rand()</code>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="co"># ----- REPRODUCIBILITY -----</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>    <span class="co"># torch.manual_seed() =&gt; sets the provided value as the seed for generating the next random tensor value to ensuring reproducibility</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>        <span class="co"># note that this method must be called EVERY TIME we want to invoke the torch.rand() method to reassign the user-defined seed value</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a>torch.manual_seed(RANDOM_SEED) <span class="co"># assign a seed value</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a>random_tensor_1 <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a>torch.manual_seed(RANDOM_SEED) <span class="co"># assign a seed value</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>random_tensor_2 <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a>random_tensor_1 <span class="op">==</span> random_tensor_2 <span class="co"># this evaluates to True</span></span></code></pre></div>
<h2 id="doing-actual-things-with-tensors">Doing actual things with Tensors</h2>
<p><em>“Enough yapping, I want to build something.”</em></p>
<h3 id="encode-an-image-to-a-tensor">Encode an Image to a Tensor</h3>
<p>The model will be trained on the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>.</p>
<ol type="1">
<li>Split the image into its RGB <em>(red green blue)</em> color channels.</li>
<li>Represent that as a tensor with the shape <em>(<code>color_channels</code>, <code>image_height</code>, <code>image_width</code>)</em>.</li>
<li>Train the model using a basic convolutional neural network (CNN).</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="co"># ----- PREPARATION WORK -----</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a><span class="co"># --- required imports ---</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a><span class="im">import</span> torchvision</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a><span class="co"># --- preprocess the CIFAR-10 image dataset ---</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a>    <span class="co"># defines transformations for the training set</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true"></a>    <span class="co"># flips the images randomly for additional fed data</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true"></a>    <span class="co"># load the actual training set and test set</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true"></a>    transforms.RandomHorizontalFlip(),  <span class="co"># randomly flip the image horizontally</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true"></a>    transforms.RandomCrop(<span class="dv">32</span>, padding<span class="op">=</span><span class="dv">4</span>),  <span class="co"># crop the image to 32x32 with padding</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true"></a>    transforms.ToTensor(),  <span class="co"># convert the image to a PyTorch tensor</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true"></a>    transforms.Normalize((<span class="fl">0.4914</span>, <span class="fl">0.4822</span>, <span class="fl">0.4465</span>), (<span class="fl">0.2023</span>, <span class="fl">0.1994</span>, <span class="fl">0.2010</span>)),  <span class="co"># normalize the image</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true"></a>])</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true"></a>trainset <span class="op">=</span> torchvision.datasets.CIFAR10(root<span class="op">=</span><span class="st">&#39;./data&#39;</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)  <span class="co"># download and transform training data</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true"></a>trainloader <span class="op">=</span> torch.utils.data.DataLoader(trainset, batch_size<span class="op">=</span><span class="dv">100</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>)  <span class="co"># create data loader for training</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true"></a>testset <span class="op">=</span> torchvision.datasets.CIFAR10(root<span class="op">=</span><span class="st">&#39;./data&#39;</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)  <span class="co"># download and transform test data</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true"></a>testloader <span class="op">=</span> torch.utils.data.DataLoader(testset, batch_size<span class="op">=</span><span class="dv">100</span>, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">2</span>)  <span class="co"># create data loader for testing</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true"></a>classes <span class="op">=</span> (<span class="st">&#39;plane&#39;</span>, <span class="st">&#39;car&#39;</span>, <span class="st">&#39;bird&#39;</span>, <span class="st">&#39;cat&#39;</span>, <span class="st">&#39;deer&#39;</span>, <span class="st">&#39;dog&#39;</span>, <span class="st">&#39;frog&#39;</span>, <span class="st">&#39;horse&#39;</span>, <span class="st">&#39;ship&#39;</span>, <span class="st">&#39;truck&#39;</span>)  <span class="co"># class labels</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true"></a><span class="co"># --- specifying CNN architecture ---</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true"></a><span class="kw">class</span> SimpleCNN(nn.Module):  </span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true"></a>        <span class="bu">super</span>(SimpleCNN, <span class="va">self</span>).<span class="fu">__init__</span>()  <span class="co"># initialize the parent class</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)  <span class="co"># first convolutional layer</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)  <span class="co"># second convolutional layer</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)  <span class="co"># max pooling layer</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">64</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>, <span class="dv">512</span>)  <span class="co"># first fully connected layer</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>)  <span class="co"># second fully connected layer</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))  <span class="co"># apply first conv layer + ReLU + pooling</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))  <span class="co"># apply second conv layer + ReLU + pooling</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">64</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>)  <span class="co"># flatten the tensor</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))  <span class="co"># apply first FC layer + ReLU</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)  <span class="co"># apply second FC layer</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true"></a>        <span class="cf">return</span> x</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true"></a><span class="co"># ----- EXECUTION CODE -----</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true"></a>net <span class="op">=</span> SimpleCNN()  <span class="co"># instantiate an instance of the CNN model</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()  <span class="co"># define the loss function</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true"></a>optimizer <span class="op">=</span> optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)  <span class="co"># define the optimizer</span></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true"></a><span class="co"># --- training loop ---</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):  <span class="co"># loop over the dataset multiple times</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span>  <span class="co"># initialize loss</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader, <span class="dv">0</span>):  <span class="co"># iterate through batches</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true"></a></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true"></a>        inputs, labels <span class="op">=</span> data  <span class="co"># get the inputs and labels</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true"></a>        optimizer.zero_grad()  <span class="co"># zero the parameter gradients</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true"></a>        outputs <span class="op">=</span> net(inputs)  <span class="co"># forward pass</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true"></a>        loss <span class="op">=</span> criterion(outputs, labels)  <span class="co"># compute loss</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true"></a>        loss.backward()  <span class="co"># backward pass</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true"></a>        optimizer.step()  <span class="co"># update weights</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true"></a>        running_loss <span class="op">+=</span> loss.item()  <span class="co"># accumulate loss</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:  <span class="co"># print every 100 mini-batches</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true"></a>            <span class="bu">print</span>(<span class="ss">f&#39;[</span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, </span><span class="sc">{i</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">] loss: </span><span class="sc">{</span>running_loss <span class="op">/</span> <span class="dv">100</span><span class="sc">:.3f}</span><span class="ss">&#39;</span>)  <span class="co"># print loss</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span>  <span class="co"># reset running loss</span></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true"></a></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&#39;we have finished training the model hooray!&#39;</span>)  <span class="co"># indicate end of training loop</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true"></a><span class="co"># --- evaluate accuracy of the model ---</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true"></a>total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># turn off gradient computation</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true"></a>    <span class="cf">for</span> data <span class="kw">in</span> testloader:  <span class="co"># iterate through test data</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true"></a>        images, labels <span class="op">=</span> data  <span class="co"># get the images and labels</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true"></a>        outputs <span class="op">=</span> net(images)  <span class="co"># forward pass</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)  <span class="co"># get the predicted labels</span></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true"></a>        total <span class="op">+=</span> labels.size(<span class="dv">0</span>)  <span class="co"># update total count</span></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true"></a>        correct <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()  <span class="co"># update correct count</span></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true"></a></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&#39;accuracy of the network on the 10000 test images: </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> correct <span class="op">/</span> total<span class="sc">}</span><span class="ss"> %&#39;</span>)  <span class="co"># model accuracy</span></span></code></pre></div>
<h2 id="more-on">More on</h2>
<h3 id="core-resources">Core resources</h3>
<ul>
<li><a href="https://pytorch.org/get-started/locally/">install pytorch locally</a></li>
<li><a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/SETUP.md">getting setup with pytorch</a></li>
<li><a href="https://pytorch.org/">pytorch.org</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">pytorch documentation</a></li>
<li><a href="https://paperswithcode.com/trends">papers with code trends</a></li>
<li><a href="https://www.reddit.com/r/MLQuestions/comments/112sege/pytorch_vs_tensorflow/">why pytorch over tensorflow</a></li>
<li><a href="https://www.learnpytorch.io/">zero to mastery: learn pytorch for deep learning</a></li>
</ul>
<h3 id="additional-resources">Additional resources</h3>
<ul>
<li><a href="https://colab.google/">colab.google</a></li>
<li><a href="https://aws.amazon.com/">aws.amazon</a></li>
<li><a href="https://www.tensorflow.org/">tensorflow.org</a></li>
<li><a href="https://keras.io/">keras.io</a></li>
<li><a href="https://github.com/meta-llama/llama">llama2</a></li>
</ul>
<h3 id="prerequisite-knowledge">Prerequisite knowledge</h3>
<ul>
<li><a href="https://learnxinyminutes.com/docs/python/">learn python in y minutes</a></li>
<li><a href="https://numpy.org/">numpy</a></li>
<li><a href="https://pandas.pydata.org/">pandas</a></li>
<li><a href="https://matplotlib.org/">matplotlib</a></li>
<li><a href="https://www.mathsisfun.com/algebra/matrix-multiplying.html">how to find dot product</a></li>
<li><a href="https://aws.amazon.com/what-is-cloud-computing/">what is cloud computing</a></li>
</ul>
</body>
</html>
