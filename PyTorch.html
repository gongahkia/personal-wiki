<h1><code>PyTorch</code></h1>
<p>Deep learning in Python.</p>
<h2>Introduction</h2>
<h3>Definitions</h3>
<ol>
<li>Deep learning: program infers relationships between user-defined <em>inputs</em> and <em>outputs</em> (supervised learning)</li>
<li>Deep learning is good for
<ul>
<li>problems with long lists of rules that would be difficult to hardcode</li>
<li>continually changing environments</li>
<li>large unstructured datasets with unclear patterns</li>
</ul>
</li>
<li>Deep learning is mostly achieved through a multilayered neural network <em>(hence deep)</em></li>
<li>Neural networks work by
<ol>
<li><strong>Input layer</strong>: <em>inputs</em> are numerically encoded into multi-dimensional vectors</li>
<li><strong>Hidden layer</strong>: vectors are processed by the many hidden layers, program finds patterns in the vectors</li>
<li><strong>Output layer</strong>: <em>output</em> is returned in the form of multi-dimensional vectors</li>
</ol>
</li>
<li>There are multiple types of neural networks
<ul>
<li>convolutional neural network (images)</li>
<li>transformer (NLP)</li>
</ul>
</li>
<li>There are a few kinds of learning
<ul>
<li>supervised learning: BOTH <em>inputs</em> and <em>outputs</em> are specified for the program</li>
<li>unsupervised / self-supervised learning: ONLY <em>input</em> is specified for the program</li>
<li>transfer learning: one program's <em>output</em> is FED to another program as <em>input</em></li>
<li>reinforcement learning: program is REWARDED for <em>ideal</em> behaviour and discouraged from <em>unideal</em> behaviour</li>
</ul>
</li>
<li>General workflow of building a deep learning program is
<ol>
<li>convert raw data to <em>input tensors</em></li>
<li>build a model
<ul>
<li>pick a loss function and optimizer</li>
<li>create the training loop</li>
</ul>
</li>
<li>tweak the model to fit the <em>data</em> and make a prediction with <em>output tensors</em></li>
<li>evaluate the model</li>
<li>improve the model through iterative experimentation</li>
<li>save and reload</li>
</ol>
</li>
<li>Tensor: any numerical representation of data <em>(most commonly multi-dimensional vectors)</em></li>
<li>There are different kinds of tensors
<ol>
<li>scalar: a single number of <em>0 dimensions</em></li>
<li>vector: a number with a direction of <em>1 dimension</em></li>
<li>matrix: a <em>2-dimensional</em> array of numbers</li>
<li>tensor: a <em>n-dimensional</em> array of numbers</li>
</ol>
</li>
<li>Random tensors: important because neural networks take in tensors full of <em>random numbers</em> and then adjust those numbers via tensor operations (addition, subtraction, simple, element and matrix multiplication, division) to <strong>better represent</strong> data</li>
</ol>
<h3>Quickstart</h3>
<pre><code class="language-py"><span class="hljs-comment"># ----- QUICKSTART -----</span>
    <span class="hljs-comment"># %%time =&gt; CPU time and Wall time for the execution of a given Jupyter notebook cell</span>
    <span class="hljs-comment"># torch.__version__ =&gt; current PyTorch version</span>
    <span class="hljs-comment"># torch.tensor() =&gt; initialises a tensor object literal, and can receive additional arguments</span>
        <span class="hljs-comment"># dtype =&gt; specifies the datatype of each element of the tensor</span>
            <span class="hljs-comment"># None</span>
            <span class="hljs-comment"># .bool =&gt; True, False</span>
            <span class="hljs-comment"># .float16</span>
            <span class="hljs-comment"># .float32 (assigned by default)</span>
            <span class="hljs-comment"># .float64</span>
            <span class="hljs-comment"># .complex32</span>
            <span class="hljs-comment"># .complex64</span>
            <span class="hljs-comment"># .complex128</span>
            <span class="hljs-comment"># .int8</span>
            <span class="hljs-comment"># .int16</span>
            <span class="hljs-comment"># .int32</span>
            <span class="hljs-comment"># .int64</span>
            <span class="hljs-comment"># .uint8</span>
            <span class="hljs-comment"># .uint16</span>
            <span class="hljs-comment"># .uint32</span>
            <span class="hljs-comment"># .uint64</span>
            <span class="hljs-comment"># .quint8</span>
            <span class="hljs-comment"># .qint8</span>
            <span class="hljs-comment"># .qint32</span>
            <span class="hljs-comment"># .quint4x2</span>
            <span class="hljs-comment"># .float8_e4m3fn</span>
            <span class="hljs-comment"># .float8_e5m2</span>
        <span class="hljs-comment"># device =&gt; specifies the device each tensor lives on</span>
            <span class="hljs-comment"># cpu</span>
            <span class="hljs-comment"># cuda</span>
            <span class="hljs-comment"># mps</span>
            <span class="hljs-comment"># xpu</span>
            <span class="hljs-comment"># xla</span>
            <span class="hljs-comment"># meta</span>
        <span class="hljs-comment"># requires_grad =&gt; specifies whether PyTorch should track the gradient of a tensor when it undergoes numerical calculations</span>
            <span class="hljs-comment"># True</span>
            <span class="hljs-comment"># False</span>
    <span class="hljs-comment"># torch.rand() =&gt; initialises a random tensor object of the specified torch.Size()</span>
    <span class="hljs-comment"># torch.zeros() =&gt; initialises a tensor of all zeros of the specified torch.Size(), most commonly used to create a mask</span>
    <span class="hljs-comment"># torch.ones() =&gt; initialises a tensor of all ones of the specified torch.Size(), most commonly used to create a mask</span>
    <span class="hljs-comment"># torch.zeros_like =&gt; initialises a tensor of all zeros of the torch.Size() from another specified tensor</span>
    <span class="hljs-comment"># torch.ones_like =&gt; initialises a tensor of all ones of the torch.Size() from another specified tensor</span>
    <span class="hljs-comment"># torch.arange(start, end, step) # initialises a tensor object literal from a range created from the specified start, end and step</span>
    <span class="hljs-comment"># .item() =&gt; called on a tensor object, which is then returned as a value literal (integer, list literal etc.)</span>
    <span class="hljs-comment"># .ndim =&gt; called on a tensor object to return the number of dimensions a given tensor has</span>
        <span class="hljs-comment"># observe that the rule of thumb is one dimension is added for every degree of [] square bracket nesting within a tensor object</span>
    <span class="hljs-comment"># .shape =&gt; recursive call on a tensor object to return the number of list elements within a given tensor</span>
    <span class="hljs-comment"># .dtype =&gt; method that returns the datatype of the specified variable it is called upon, PyTorch assigns the default datatype of .float32 if unspecified</span>
    <span class="hljs-comment"># .device =&gt; method that returns the current device of a given tensor object</span>

<span class="hljs-comment"># --- DATA SCIENCE PACKAGES TO IMPORT ---</span>

<span class="hljs-keyword">import</span> torch 
<span class="hljs-keyword">import</span> pandas
<span class="hljs-keyword">import</span> numpy
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> p

<span class="hljs-built_in">print</span>(torch.__version__) <span class="hljs-comment"># display the current PyTorch version</span>

<span class="hljs-comment"># --- USER-DEFINED TENSORS ---</span>

scalar = torch.tensor(<span class="hljs-number">7</span>) <span class="hljs-comment"># initialise a scalar tensor object</span>
scalar.item() <span class="hljs-comment"># returns 7</span>
scalar.ndim <span class="hljs-comment"># returns 0 dimensions</span>
scalar.shape <span class="hljs-comment"># returns torch.Size([]) to indicate that there are no list elements</span>

vector = torch.tensor([<span class="hljs-number">7</span>, <span class="hljs-number">7</span>]) <span class="hljs-comment"># intialise a vector tensor object</span>
vector.item() <span class="hljs-comment"># returns [7, 7]</span>
vector.ndim <span class="hljs-comment"># returns 1 dimension</span>
vector.shape <span class="hljs-comment"># returns torch.Size(2) to indicate 2 elements</span>

<span class="hljs-comment"># - NOTE -</span>
    <span class="hljs-comment"># by convention, scalar and vector variables are declared in lowercase while matrix and tensor variables are declared in UPPERCASE</span>

MATRIX = torch.tensor(
    [[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>], 
    [<span class="hljs-number">9</span>, <span class="hljs-number">10</span>]]
)
MATRIX.ndim() <span class="hljs-comment"># returns 2 dimensions</span>
MATRIX.shape <span class="hljs-comment"># returns torch.Size([2, 2]) to indicate 2 list elements each containing 2 elements</span>

TENSOR = torch.tensor(
    [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
    [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
    [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]]
)
TENSOR.ndim <span class="hljs-comment"># returns 3 dimensions</span>
TENSOR.shape <span class="hljs-comment"># returns torch.Size([1, 3, 3]) to indicate 1 list element which contains 3 list elements which contain 3 elements each</span>

WATERMELON = torch.tensor(
    [[[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
    [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>],
    [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
    [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>],
    [<span class="hljs-number">9</span>, <span class="hljs-number">10</span>],
    [<span class="hljs-number">11</span>, <span class="hljs-number">12</span>]]]]
)
WATERMELON.ndim <span class="hljs-comment"># returns 4 dimensions</span>
WATERMELON.shape <span class="hljs-comment"># returns torch.Size([1, 1, 6, 2]) to indicate 1 list element that contains 1 list element that contains 6 list elements which then contains 2 elements each</span>

<span class="hljs-comment"># --- RANDOM TENSOR ---</span>

RANDOM_TENSOR = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>) <span class="hljs-comment"># initialises a random tensor of torch.Size([3, 4])</span>
RANDOM_TENSOR.ndim <span class="hljs-comment"># returns 2 dimensions</span>

RANDOM_IMAGE_SIZE_TENSOR = torch.rand(size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>)) <span class="hljs-comment"># initialises a random tensor with a similar shape to an image tensor, specifying the height, width and color channel</span>
RANDOM_IMAGE_SIZE_TENSOR.ndim <span class="hljs-comment"># returns 3 dimensions as we specified above</span>
RANDOM_IMAGE_SIZE_TENSOR.shape <span class="hljs-comment"># returns torch.Size([224, 224, 3]) as we specified above</span>

<span class="hljs-comment"># --- TENSOR OF ALL 0s ---</span>

ZERO_TENSOR = torch.zeros(size=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)) <span class="hljs-comment"># initialises a tensor of all zeros of the torch.Size([3, 4])</span>

<span class="hljs-comment"># --- TENSOR OF ALL 1s ---</span>

ONE_TENSOR = torch.ones(size=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)) <span class="hljs-comment"># initialises a tensor of all ones of the torch.Size([3, 4])</span>

<span class="hljs-comment"># --- RANGE TENSOR ---</span>

zero_to_nine = torch.arange(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>) <span class="hljs-comment"># initialises the tensor object literal tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
two_to_eleven = torch.arange(start=<span class="hljs-number">2</span>, end=<span class="hljs-number">11</span>, step=<span class="hljs-number">1</span>) <span class="hljs-comment"># initialises the tensor object literal tensor([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])</span>

<span class="hljs-comment"># --- TENSORS LIKE ---</span>

ten_zeroes = torch.zeros_like(<span class="hljs-built_in">input</span>=zero_to_nine) <span class="hljs-comment"># initialises a zero tensor of the same shape as the specified input tensor</span>
ten_ones = torch.ones_like(<span class="hljs-built_in">input</span>=zero_to_nine) <span class="hljs-comment"># initialises a one tensor of the same shape as the specified input tensor</span>
</code></pre>
<h3>Tensor operations</h3>
<pre><code class="language-py"><span class="hljs-comment"># ----- TENSOR OPERATIONS -----</span>

<span class="hljs-comment"># --- ARITHMETIC METHODS ---</span>
    <span class="hljs-comment"># + =&gt; addition applied to each element of the tensor</span>
    <span class="hljs-comment"># - =&gt; subtraction applied to each element of the tensor</span>
    <span class="hljs-comment"># * =&gt; simple multiplication of a matrix against a scalar number</span>
    <span class="hljs-comment"># / =&gt; division applied to each element of the tensor</span>
    <span class="hljs-comment"># * =&gt; ELEMENT multiplication of two matrices, where each element of a matrix is multipled against its corresponding element in the other matrix</span>
    <span class="hljs-comment"># torch.matmul() =&gt; MATRIX multiplication that finds the DOT PRODUCT of two specified matrices by multiplying them together</span>
        <span class="hljs-comment"># observe that matrix multiplication must satisfy the following 2 rules</span>
            <span class="hljs-comment"># 1. inner dimensions of the two matrices must match</span>
                <span class="hljs-comment"># torch.matmul(torch.rand(2, 3), torch.rand(2, 3)) WON&#x27;T work</span>
                <span class="hljs-comment"># torch.matmul(torch.rand(2, 3), torch.rand(3, 2)) WILL work</span>
                <span class="hljs-comment"># torch.matmul(torch.rand(3, 2), torch.rand(2, 3)) WILL work</span>
            <span class="hljs-comment"># 2. result matrix must have the shape of the outer dimensions</span>
                <span class="hljs-comment"># torch.matmul(torch.rand(2, 3), torch.rand(3, 2)) results in torch.Size([2, 2]) so this WILL work</span>
                <span class="hljs-comment"># torch.matmul(torch.rand(3, 2), torch.rand(2, 3)) results in torch.Size([3, 3]) so this WILL work</span>

<span class="hljs-comment"># --- AGGREGATOR METHODS ---</span>
    <span class="hljs-comment"># torch.min() =&gt; finds the element with the minimum value in a given tensor </span>
    <span class="hljs-comment"># torch.max() =&gt; finds the element with the maximum value in a given tensor </span>
    <span class="hljs-comment"># torch.mean() =&gt; finds the average value of all elements within a given tensor, note the element datatype must be a floating or complex type</span>
    <span class="hljs-comment"># torch.sum() =&gt; finds the sum of all elements within a given tensor</span>
    <span class="hljs-comment"># torch.argmin() =&gt; finds the index of the element with the minimum value in a given tensor </span>
    <span class="hljs-comment"># torch.argmax() =&gt; finds the index of the element with the maximum value in a given tensor </span>

<span class="hljs-comment"># --- MANIPULATION METHODS ---</span>
    <span class="hljs-comment"># one of the most common issues relating to tensors arises due to shape and dimension, which is combated by</span>
        <span class="hljs-comment"># reshaping =&gt; reshaping an input tensor to a specified shape</span>
        <span class="hljs-comment"># view =&gt; returns a view of an input tensor in a specified shape while pointing to the same place in memory as the original tensor</span>
        <span class="hljs-comment"># stacking =&gt; combine multiple tensors together in a vertical (vstack) or horizontal (hstack) stack</span>
        <span class="hljs-comment"># squeeze =&gt; remove all 1 dimensions from a given tensor</span>
        <span class="hljs-comment"># unsqueeze =&gt; add a 1 dimension to a given tensor</span>
        <span class="hljs-comment"># permute =&gt; returns a view of an input tensor with its dimensions swapped in a certain way</span>
    <span class="hljs-comment"># the PyTorch methods are as follows</span>
        <span class="hljs-comment"># .T =&gt; method that tranposes the shape of the specified tensor by switching its dimensions (axis), particularly useful for when tensor shape errors occur</span>
        <span class="hljs-comment"># .reshape() =&gt; method that reshapes a specified tensor to the new provided dimensions, note that the total corresponding number of elements must stay the same across a reshape</span>
        <span class="hljs-comment"># .view() =&gt; method that merely displays an existing tensor differenly according to the new provided dimensions whilst pointing to the original tensor&#x27;s memory address (which means changing the new tensor variable assigned to a view changes the value of the original tensor being viewed)</span>
        <span class="hljs-comment"># torch.stack() =&gt; method that stacks multiple provided tensors together, with an optional dim argument that further allows augmentation of the desired number of dimensions within the new tensor</span>
            <span class="hljs-comment"># torch.vstack</span>
            <span class="hljs-comment"># torch.hstack</span>
        <span class="hljs-comment"># torch.squeeze() =&gt; method that removes all SINGLE dimensions from a given tensor</span>
        <span class="hljs-comment"># torch.unsqueeze() =&gt; method that adds a SINGLE dimension to a given tensor, with a dim argument that further specifies which dimension to add the single dimension at</span>
        <span class="hljs-comment"># torch.permute() =&gt; method that rearranges the dimensions of a given tensor to a new specified order and returns a VIEW of that new tensor (which means changing the new tensor variable assigned to a permute changes the value of the original tensor being permuted)</span>

<span class="hljs-comment"># --- SELECTION METHODS ---</span>
    <span class="hljs-comment"># [] =&gt; indexing in PyTorch is similar to indexing in Python and NumPy, where list values are zero-indexed and can have nested calls</span>
    <span class="hljs-comment"># : =&gt; specifies to select ALL of a given target dimension</span>

<span class="hljs-comment"># - NOTE -</span>
    <span class="hljs-comment"># recall that we have to reassign the result of a tensor operation to a variable for the value to be stored, similar to anywhere else in Python and most other programming languages really</span>
    <span class="hljs-comment"># the examples below are selected samples of the above methods and are not comprehensive, more detailed use cases can be found in PyTorch&#x27;s documentation</span>

<span class="hljs-keyword">import</span> torch

<span class="hljs-comment"># intialisation of tensor object literals</span>
tensor = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) 
another_tensor = torch.tensor(
    [[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>],
    [<span class="hljs-number">9</span>, <span class="hljs-number">10</span>],
    [<span class="hljs-number">11</span>, <span class="hljs-number">12</span>]]
)
yet_another_tensor = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>) <span class="hljs-comment"># initialises the tensor object literal tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
final_tensor = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>) <span class="hljs-comment"># initialises the tensor object literal tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]) </span>

tensor + <span class="hljs-number">10</span> <span class="hljs-comment"># addition that evaluates to the tensor object literal tensor([101, 102, 103])</span>
tensor - <span class="hljs-number">10</span> <span class="hljs-comment"># subtraction that evaluates to the tensor object literal tensor([-9, -8, -7])</span>
tensor * <span class="hljs-number">10</span> <span class="hljs-comment"># simple multiplication evaluates to the tensor object literal tensor([10, 20, 30])</span>
tensor / <span class="hljs-number">10</span> <span class="hljs-comment"># divison evaluates to the tensor object literal tensor([0.1, 0.2, 0.3])</span>

tensor * tensor <span class="hljs-comment"># element multiplication evaluates to the tensor object literal tensor([1, 4, 9])</span>
torch.matmul(tensor, tensor) <span class="hljs-comment"># matrix multiplication evalutes to the dot product value torch(14)</span>

another_tensor.T <span class="hljs-comment"># transpose operation that evaluates to the tensor object literal tensor([[7, 9, 11], [8, 10, 12]])</span>
another_tensor.T.shape <span class="hljs-comment"># transpose operation means this will now return the torch.Size([2, 3])</span>

yet_another_tensor.shape <span class="hljs-comment"># returns torch.Size([9])</span>
reshaped_tensor = yet_another_tensor.reshape(<span class="hljs-number">9</span>, <span class="hljs-number">1</span>)
reshaped_tensor.shape <span class="hljs-comment"># returns torch.Size([9, 1])</span>

view_tensor = yet_another_tensor.view(<span class="hljs-number">9</span>, <span class="hljs-number">1</span>)
view_tensor.shape <span class="hljs-comment"># returns torch.Size([9, 1]), but note that modifying view_tensor will also modify the value of yet_another_tensor</span>

stack_tensor = torch.stack([yet_another_tensor, yet_another_tensor, yet_another_tensor, yet_another_tensor], dim = <span class="hljs-number">0</span>) <span class="hljs-comment"># restacks the tensor according to dimension 0</span>
stack_tensor = torch.stack([yet_another_tensor, yet_another_tensor, yet_another_tensor, yet_another_tensor], dim = <span class="hljs-number">1</span>) <span class="hljs-comment"># restacks the tensor according to dimension 1</span>

final_tensor.shape <span class="hljs-comment"># unmodified tensor will return torch.Size([1, 3, 3])</span>
final_tensor[<span class="hljs-number">0</span>] <span class="hljs-comment"># evaluates to tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span>
final_tensor[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] <span class="hljs-comment"># evaluates to tensor([1, 2, 3])</span>
final_tensor[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] <span class="hljs-comment"># evaluates to tensor(1)</span>
</code></pre>
<h3>PyTorch and NumPy</h3>
<pre><code class="language-py"><span class="hljs-comment"># ----- NUMPY -----</span>
    <span class="hljs-comment"># torch.from_numpy() =&gt; receives NumPy data and converts it to a PyTorch tensor</span>
    <span class="hljs-comment"># torch.Tensor.numpy() =&gt; receives a PyTorch tensor and converts it to NumPy data</span>
    <span class="hljs-comment"># observe that NumPy&#x27;s default datatype is float64 while PyTorch&#x27;s default datatype is float32</span>

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> numpy

array = numpy.arange(<span class="hljs-number">1.0</span>, <span class="hljs-number">8.0</span>) <span class="hljs-comment"># initialise a NumPy array</span>
tensor = torch.from_numpy(array) <span class="hljs-comment"># convert that NumPy array to a PyTorch tensor</span>
back_to_array = torch.Tensor.numpy(tensor) <span class="hljs-comment"># converting that PyTorch tensor back to a NumPy array</span>
</code></pre>
<h3>Reproducibility</h3>
<p>Introduce a <strong>random seed</strong> to flavour the randomness of <code>torch.rand()</code>.</p>
<pre><code class="language-py"><span class="hljs-comment"># ----- REPRODUCIBILITY -----</span>
    <span class="hljs-comment"># torch.manual_seed() =&gt; sets the provided value as the seed for generating the next random tensor value to ensuring reproducibility</span>
        <span class="hljs-comment"># note that this method must be called EVERY TIME we want to invoke the torch.rand() method to reassign the user-defined seed value</span>

<span class="hljs-keyword">import</span> torch

RANDOM_SEED = <span class="hljs-number">42</span>
torch.manual_seed(RANDOM_SEED) <span class="hljs-comment"># assign a seed value</span>
random_tensor_1 = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
torch.manual_seed(RANDOM_SEED) <span class="hljs-comment"># assign a seed value</span>
random_tensor_2 = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)

random_tensor_1 == random_tensor_2 <span class="hljs-comment"># this evaluates to True</span>
</code></pre>
<h2>Doing actual things with Tensors</h2>
<p><em>&quot;Enough yapping, I want to build something.&quot;</em></p>
<h3>Encode an Image to a Tensor</h3>
<p>The model will be trained on the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>.</p>
<ol>
<li>Split the image into its RGB <em>(red green blue)</em> color channels.</li>
<li>Represent that as a tensor with the shape <em>(<code>color_channels</code>, <code>image_height</code>, <code>image_width</code>)</em>.</li>
<li>Train the model using a basic convolutional neural network (CNN).</li>
</ol>
<pre><code class="language-py"><span class="hljs-comment"># ----- PREPARATION WORK -----</span>

<span class="hljs-comment"># --- required imports ---</span>

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-comment"># --- preprocess the CIFAR-10 image dataset ---</span>
    <span class="hljs-comment"># defines transformations for the training set</span>
    <span class="hljs-comment"># flips the images randomly for additional fed data</span>
    <span class="hljs-comment"># load the actual training set and test set</span>

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  <span class="hljs-comment"># randomly flip the image horizontally</span>
    transforms.RandomCrop(<span class="hljs-number">32</span>, padding=<span class="hljs-number">4</span>),  <span class="hljs-comment"># crop the image to 32x32 with padding</span>
    transforms.ToTensor(),  <span class="hljs-comment"># convert the image to a PyTorch tensor</span>
    transforms.Normalize((<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), (<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>)),  <span class="hljs-comment"># normalize the image</span>
])

trainset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>, transform=transform)  <span class="hljs-comment"># download and transform training data</span>
trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)  <span class="hljs-comment"># create data loader for training</span>
testset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>, transform=transform)  <span class="hljs-comment"># download and transform test data</span>
testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)  <span class="hljs-comment"># create data loader for testing</span>

classes = (<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>)  <span class="hljs-comment"># class labels</span>

<span class="hljs-comment"># --- specifying CNN architecture ---</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleCNN</span>(nn.Module):  
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-built_in">super</span>(SimpleCNN, <span class="hljs-variable language_">self</span>).__init__()  <span class="hljs-comment"># initialize the parent class</span>
        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)  <span class="hljs-comment"># first convolutional layer</span>
        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)  <span class="hljs-comment"># second convolutional layer</span>
        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>)  <span class="hljs-comment"># max pooling layer</span>
        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">8</span> * <span class="hljs-number">8</span>, <span class="hljs-number">512</span>)  <span class="hljs-comment"># first fully connected layer</span>
        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>)  <span class="hljs-comment"># second fully connected layer</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)))  <span class="hljs-comment"># apply first conv layer + ReLU + pooling</span>
        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)))  <span class="hljs-comment"># apply second conv layer + ReLU + pooling</span>
        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">64</span> * <span class="hljs-number">8</span> * <span class="hljs-number">8</span>)  <span class="hljs-comment"># flatten the tensor</span>
        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))  <span class="hljs-comment"># apply first FC layer + ReLU</span>
        x = <span class="hljs-variable language_">self</span>.fc2(x)  <span class="hljs-comment"># apply second FC layer</span>
        <span class="hljs-keyword">return</span> x

<span class="hljs-comment"># ----- EXECUTION CODE -----</span>

net = SimpleCNN()  <span class="hljs-comment"># instantiate an instance of the CNN model</span>

criterion = nn.CrossEntropyLoss()  <span class="hljs-comment"># define the loss function</span>
optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)  <span class="hljs-comment"># define the optimizer</span>

<span class="hljs-comment"># --- training loop ---</span>

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):  <span class="hljs-comment"># loop over the dataset multiple times</span>
    running_loss = <span class="hljs-number">0.0</span>  <span class="hljs-comment"># initialize loss</span>
    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader, <span class="hljs-number">0</span>):  <span class="hljs-comment"># iterate through batches</span>

        inputs, labels = data  <span class="hljs-comment"># get the inputs and labels</span>

        optimizer.zero_grad()  <span class="hljs-comment"># zero the parameter gradients</span>

        outputs = net(inputs)  <span class="hljs-comment"># forward pass</span>
        loss = criterion(outputs, labels)  <span class="hljs-comment"># compute loss</span>
        loss.backward()  <span class="hljs-comment"># backward pass</span>
        optimizer.step()  <span class="hljs-comment"># update weights</span>

        running_loss += loss.item()  <span class="hljs-comment"># accumulate loss</span>

        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">99</span>:  <span class="hljs-comment"># print every 100 mini-batches</span>
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[<span class="hljs-subst">{epoch + <span class="hljs-number">1</span>}</span>, <span class="hljs-subst">{i + <span class="hljs-number">1</span>}</span>] loss: <span class="hljs-subst">{running_loss / <span class="hljs-number">100</span>:<span class="hljs-number">.3</span>f}</span>&#x27;</span>)  <span class="hljs-comment"># print loss</span>
            running_loss = <span class="hljs-number">0.0</span>  <span class="hljs-comment"># reset running loss</span>

<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;we have finished training the model hooray!&#x27;</span>)  <span class="hljs-comment"># indicate end of training loop</span>

<span class="hljs-comment"># --- evaluate accuracy of the model ---</span>

correct = <span class="hljs-number">0</span>
total = <span class="hljs-number">0</span>
<span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># turn off gradient computation</span>
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> testloader:  <span class="hljs-comment"># iterate through test data</span>
        images, labels = data  <span class="hljs-comment"># get the images and labels</span>
        outputs = net(images)  <span class="hljs-comment"># forward pass</span>
        _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)  <span class="hljs-comment"># get the predicted labels</span>
        total += labels.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># update total count</span>
        correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()  <span class="hljs-comment"># update correct count</span>

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;accuracy of the network on the 10000 test images: <span class="hljs-subst">{<span class="hljs-number">100</span> * correct / total}</span> %&#x27;</span>)  <span class="hljs-comment"># model accuracy</span>
</code></pre>
<h2>More on</h2>
<h3>Core resources</h3>
<ul>
<li><a href="https://pytorch.org/get-started/locally/">install pytorch locally</a></li>
<li><a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/SETUP.md">getting setup with pytorch</a></li>
<li><a href="https://pytorch.org/">pytorch.org</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">pytorch documentation</a></li>
<li><a href="https://paperswithcode.com/trends">papers with code trends</a></li>
<li><a href="https://www.reddit.com/r/MLQuestions/comments/112sege/pytorch_vs_tensorflow/">why pytorch over tensorflow</a></li>
<li><a href="https://www.learnpytorch.io/">zero to mastery: learn pytorch for deep learning</a></li>
</ul>
<h3>Additional resources</h3>
<ul>
<li><a href="https://colab.google/">colab.google</a></li>
<li><a href="https://aws.amazon.com/">aws.amazon</a></li>
<li><a href="https://www.tensorflow.org/">tensorflow.org</a></li>
<li><a href="https://keras.io/">keras.io</a></li>
<li><a href="https://github.com/meta-llama/llama">llama2</a></li>
</ul>
<h3>Prerequisite knowledge</h3>
<ul>
<li><a href="https://learnxinyminutes.com/docs/python/">learn python in y minutes</a></li>
<li><a href="https://numpy.org/">numpy</a></li>
<li><a href="https://pandas.pydata.org/">pandas</a></li>
<li><a href="https://matplotlib.org/">matplotlib</a></li>
<li><a href="https://www.mathsisfun.com/algebra/matrix-multiplying.html">how to find dot product</a></li>
<li><a href="https://aws.amazon.com/what-is-cloud-computing/">what is cloud computing</a></li>
</ul>
