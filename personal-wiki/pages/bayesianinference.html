<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="Wiki Note: Bayesian Inference - Gabriel Ong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Gabriel Ong">
  <meta name="robots" content="index, follow">
  
  <meta property="og:title" content="Bayesian Inference | Gabriel Ong Wiki">
  <meta property="og:description" content="Wiki Note: Bayesian Inference - Gabriel Ong">
  <meta property="og:type" content="article">
  
  <meta property="og:image" content="https://gabrielongzm.com/asset/portrait/gong-2.png">
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Bayesian Inference | Gabriel Ong Wiki">
  <meta name="twitter:description" content="Wiki Note: Bayesian Inference - Gabriel Ong">
  
  <link rel="stylesheet" href="../../style.css">
  <link rel="preload" href="https://res.hajimehoshi.com/fonts/SuisseIntl-Regular-WebXL.woff2" as="font" crossorigin="anonymous">
  
  <link rel="preload" href="https://res.hajimehoshi.com/fonts/SuisseIntlMono-Regular-WebXL.woff2" as="font" crossorigin="anonymous">

  <style>.thin-space:after{content:"\2006"}</style>
  
  <style>pre { overflow-x: auto; max-width: 100%; }</style>

  <script src="../script.js" defer></script>
  <title>Bayesian Inference | Gabriel Ong Wiki</title>
  <link rel="shortcut icon" href="../asset/blob.ico" type="image/x-icon">
</head>
<body>
  <div id="click-container"></div>
  <input type="button" id="dark-mode">
  <label for="dark-mode">
    <img id="infinityButton" src="../asset/roller.png" height="24" width="24"/>
  </label>
  <main>
    <article class="overallArticleTags">
      
      <section class="note-header">
        <h2>Bayesian Inference</h2>
        <dl>
          <dt>File size</dt>
          <dd>7.2KB</dd>
          <dt>Lines of code</dt>
          <dd>117</dd>
        </dl>
      </section>
      <section class="note-content">
        <h1><code>Bayesian Inference</code></h1>
<p>Notes on the 3Blue1Brown series "Probabilities of probabilities," which provides an introduction to the core ideas of Bayesian inference.</p>
<h2>Introduction: The Bayesian View</h2>
<p>Bayesian inference is a framework for thinking about probability as a measure of belief in a proposition. The core idea is to update our beliefs in the light of new evidence. This is in contrast to the frequentist interpretation of probability, which sees probability as the long-run frequency of an event.</p>
<h2>Bayes' Theorem</h2>
<p>The mathematical engine of Bayesian inference is Bayes' Theorem. It tells us how to update our belief in a hypothesis ($H$) after observing some evidence ($E$).</p>
<p>$$ <br />
P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}<br />
$$</p>
<p>Let's break down the terms:<br />
*   $P(H|E)$: The <strong>Posterior</strong>. This is the probability of our hypothesis being true, given the evidence. This is what we want to calculate.<br />
*   $P(E|H)$: The <strong>Likelihood</strong>. This is the probability of observing the evidence, assuming our hypothesis is true.<br />
*   $P(H)$: The <strong>Prior</strong>. This is our initial belief in the hypothesis, before we've seen any evidence.<br />
*   $P(E)$: The <strong>Marginal Likelihood</strong>. This is the total probability of observing the evidence, under all possible hypotheses. It acts as a normalization constant.</p>
<p>In practice, we often write the theorem as:<br />
$$ <br />
\text{Posterior} \propto \text{Likelihood} \cdot \text{Prior}<br />
$$ </p>
<h2>From Numbers to Distributions</h2>
<p>In many real-world problems, we are not interested in a single probability, but a continuous range of possibilities. For example, instead of asking "is this coin fair?", we might ask "what is the probability of this coin landing heads?". This is a value that could be anywhere between 0 and 1.</p>
<p>In the Bayesian framework, we can represent our belief about this unknown probability with a <strong>probability distribution</strong>.</p>
<h2>The Beta Distribution</h2>
<p>The Beta distribution is a family of continuous probability distributions defined on the interval [0, 1]. It is a very natural choice for representing a belief about a probability. It is defined by two positive shape parameters, $\alpha$ and $\beta$.</p>
<p>$$ <br />
\text{Beta}(\alpha, \beta)<br />
$$ </p>
<ul>
<li>The mean of the distribution is $\frac{\alpha}{\alpha + \beta}$.</li>
<li>The shape of the distribution can be interpreted as representing the knowledge gained from $\alpha - 1$ "successes" and $\beta - 1$ "failures".</li>
</ul>
<p>For example, a flat prior (representing no knowledge) can be modeled with a Beta(1, 1) distribution, which is a uniform distribution.</p>
<h2>Conjugate Priors</h2>
<p>The Beta distribution has a very special relationship with the binomial distribution (which describes the number of successes in a series of independent trials). The Beta distribution is a <strong>conjugate prior</strong> for the binomial likelihood.</p>
<p>This means that if you:<br />
1.  Start with a <strong>prior</strong> belief about a probability, represented by a Beta distribution, $\text{Beta}(\alpha, \beta)$.<br />
2.  Observe new evidence in the form of $k$ successes and $n-k$ failures.<br />
3.  Your <strong>posterior</strong> belief will also be a Beta distribution, with updated parameters:<br />
    $$ <br />
    \text{Beta}(\alpha + k, \beta + n - k)<br />
    $$ </p>
<p>This makes calculations much easier and provides a very intuitive way to think about updating beliefs. You just add the number of successes to $\alpha$ and the number of failures to $\beta$.</p>
<h2>Example: Coin Flipping</h2>
<p>Suppose you want to determine the fairness of a coin.<br />
*   <strong>Prior:</strong> You have no idea if it's fair, so you start with a uniform prior, $\text{Beta}(1, 1)$.<br />
*   <strong>Evidence:</strong> You flip the coin 10 times and get 7 heads and 3 tails.<br />
*   <strong>Posterior:</strong> Your new belief about the coin's probability of landing heads is represented by the distribution $\text{Beta}(1+7, 1+3) = \text{Beta}(8, 4)$.</p>
<p>The peak of this new distribution is at $7/10 = 0.7$, which is our most likely estimate for the probability of heads. But the distribution also tells us about our uncertainty; there's still a chance the coin is fair, or even biased towards tails.</p>
<h2>Bayesian A/B Testing</h2>
<p>This framework can be applied to A/B testing. Instead of p-values, we can directly calculate the probability that variant B is better than variant A. We can model our belief about the conversion rate of each variant with a Beta distribution, update it with new data, and then compare the posterior distributions.</p>
<h2>More on</h2>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDOjmo3Y6ADm0ScWAlEXf-fp">Probabilities of probabilities: 3Blue1Brown</a></li>
<li><a href="https://seeing-theory.brown.edu/bayesian-inference/index.html">An interactive introduction to Bayesian reasoning</a></li>
<li><a href="https://greenteapress.com/wp/think-bayes/">Think Bayes by Allen B. Downey</a></li>
</ul>
      </section>

    </article>
    <footer>
      <p>&copy; 2023-<span id="current-year"></span> Gabriel Ong. All rights reserved.</p>
    </footer>
  </main>
  <div class="wrapper"></div>
  
</body>
</html>